# Alpamayo-R1 å®Œæ•´æŠ€æœ¯æ–‡æ¡£

æœ¬æ–‡æ¡£æ•´åˆäº† Alpamayo-R1 æ¨¡å‹çš„æ‰€æœ‰æŠ€æœ¯åˆ†æï¼ŒåŒ…æ‹¬æ¶æ„ã€ç»„ä»¶ã€æ¨ç†æµç¨‹ã€Attention æœºåˆ¶ç­‰è¯¦ç»†å†…å®¹ã€‚

## ğŸ“‹ å®Œæ•´ç›®å½•

### ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¨¡å‹æ¶æ„ä¸æ¨ç†æµç¨‹
1. [æ•´ä½“æ¶æ„æ¦‚è§ˆ](#1-æ•´ä½“æ¶æ„æ¦‚è§ˆ)
2. [æ ¸å¿ƒç»„ä»¶è¯¦è§£](#2-æ ¸å¿ƒç»„ä»¶è¯¦è§£)
3. [æ¨ç†æµç¨‹è¯¦è§£](#3-æ¨ç†æµç¨‹è¯¦è§£)
4. [æ•°æ®æµä¸å¼ é‡å½¢çŠ¶](#4-æ•°æ®æµä¸å¼ é‡å½¢çŠ¶)

### ç¬¬äºŒéƒ¨åˆ†ï¼šExpert Model è¯¦è§£
5. [ä»€ä¹ˆæ˜¯ Expert Modelï¼Ÿ](#5-ä»€ä¹ˆæ˜¯-expert-model)
6. [Expert Model çš„æ¶æ„](#6-expert-model-çš„æ¶æ„)
7. [Expert Model çš„ä½œç”¨](#7-expert-model-çš„ä½œç”¨)
8. [ä¸ VLM çš„å…³ç³»](#8-ä¸-vlm-çš„å…³ç³»)

### ç¬¬ä¸‰éƒ¨åˆ†ï¼šAttention æœºåˆ¶åˆ†æ
9. [Causal vs Non-Causal Attention](#9-causal-vs-non-causal-attention)
10. [VLM çš„ Causal Attention](#10-vlm-çš„-causal-attention)
11. [Expert Model çš„ Non-Causal Attention](#11-expert-model-çš„-non-causal-attention)

### ç¬¬å››éƒ¨åˆ†ï¼šVision Token Attention
12. [å›¾åƒæ•°æ®ç»„ç»‡](#12-å›¾åƒæ•°æ®ç»„ç»‡)
13. [Vision Token ç”Ÿæˆ](#13-vision-token-ç”Ÿæˆ)
14. [Vision Token Attention è§„åˆ™](#14-vision-token-attention-è§„åˆ™)
15. [View å’Œ Frame çš„é¡ºåºå…³ç³»](#15-view-å’Œ-frame-çš„é¡ºåºå…³ç³»)
16. [Vision Token Attention è¯æ®åˆ†æ](#16-vision-token-attention-è¯æ®åˆ†æ)

### ç¬¬äº”éƒ¨åˆ†ï¼šå›¾åƒå¤„ç†ä¸ Vision Encoder
17. [å›¾åƒå¤„ç†æµç¨‹è¯¦è§£](#17-å›¾åƒå¤„ç†æµç¨‹è¯¦è§£)
18. [Vision Encoder ä½ç½®ä¸è°ƒç”¨](#18-vision-encoder-ä½ç½®ä¸è°ƒç”¨)

---

# ç¬¬ä¸€éƒ¨åˆ†ï¼šæ¨¡å‹æ¶æ„ä¸æ¨ç†æµç¨‹

## 1. æ•´ä½“æ¶æ„æ¦‚è§ˆ

Alpamayo-R1 æ˜¯ä¸€ä¸ª **Vision-Language-Action (VLA)** æ¨¡å‹ï¼Œç”¨äºè‡ªåŠ¨é©¾é©¶åœºæ™¯çš„è½¨è¿¹é¢„æµ‹å’Œæ¨ç†ã€‚æ¨¡å‹é‡‡ç”¨**æ··åˆæ¶æ„**ï¼Œç»“åˆäº†è§†è§‰-è¯­è¨€æ¨¡å‹ï¼ˆVLMï¼‰å’Œä¸“å®¶æ¨¡å‹ï¼ˆExpert Modelï¼‰ã€‚

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    Alpamayo-R1 æ¨¡å‹æ¶æ„                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

è¾“å…¥å±‚
  â”œâ”€â”€ å¤šç›¸æœºå›¾åƒ (4ä¸ªç›¸æœº Ã— 4å¸§)
  â”œâ”€â”€ å†å²è½¨è¿¹ (ego_history_xyz, ego_history_rot)
  â””â”€â”€ æ—¶é—´æˆ³ä¿¡æ¯

VLM Backbone (Qwen3-VL-8B-Instruct)
  â”œâ”€â”€ è§†è§‰ç¼–ç å™¨ (å¤„ç†å¤šç›¸æœºå›¾åƒ)
  â”œâ”€â”€ è¯­è¨€æ¨¡å‹ (ç”Ÿæˆ Chain-of-Causation æ¨ç†)
  â””â”€â”€ è½¨è¿¹tokenèåˆ (fuse_traj_tokens)

Expert Model (åŸºäº VLM çš„æ–‡æœ¬é…ç½®)
  â”œâ”€â”€ Action Input Projection (action_in_proj)
  â”œâ”€â”€ Expert Transformer (å¤„ç†åŠ¨ä½œåºåˆ—)
  â””â”€â”€ Action Output Projection (action_out_proj)

Diffusion Model
  â””â”€â”€ Flow Matching / å»å™ªè¿‡ç¨‹

Action Space
  â””â”€â”€ åŠ¨ä½œç©ºé—´åˆ°è½¨è¿¹çš„è½¬æ¢

è¾“å‡ºå±‚
  â”œâ”€â”€ é¢„æµ‹è½¨è¿¹ (pred_xyz, pred_rot)
  â””â”€â”€ Chain-of-Causation æ–‡æœ¬ (CoC reasoning)
```

---

## 2. æ ¸å¿ƒç»„ä»¶è¯¦è§£

### 2.1 ReasoningVLA (åŸºç¡€ç±»)

**ä½ç½®**: `alpamayo_r1/models/base_model.py`

**åŠŸèƒ½**: æä¾› VLA æ¨¡å‹çš„åŸºç¡€æ¡†æ¶

**å…³é”®ç»„ä»¶**:
- **VLM Backbone**: åŸºäº Qwen3-VL-8B-Instruct çš„è§†è§‰-è¯­è¨€æ¨¡å‹
  - å¤„ç†å¤šæ¨¡æ€è¾“å…¥ï¼ˆå›¾åƒ + æ–‡æœ¬ï¼‰
  - æ”¯æŒè½¨è¿¹tokençš„ç‰¹æ®Šå¤„ç†
  - ä½¿ç”¨ Flash Attention 2 ä¼˜åŒ–

- **è½¨è¿¹Tokenç³»ç»Ÿ**:
  - `traj_tokenizer`: æœªæ¥è½¨è¿¹çš„tokenizer
  - `hist_traj_tokenizer`: å†å²è½¨è¿¹çš„tokenizer
  - ç‰¹æ®Štoken: `<|traj_history_start|>`, `<|traj_future_start|>`, `<|cot_start|>` ç­‰

- **TrajectoryFusionMixin**: 
  - `fuse_traj_tokens()`: å°†å†å²è½¨è¿¹ç¼–ç ä¸ºtokenå¹¶èåˆåˆ°è¾“å…¥åºåˆ—ä¸­

### 2.2 AlpamayoR1 (ä¸“å®¶æ¨¡å‹)

**ä½ç½®**: `alpamayo_r1/models/alpamayo_r1.py`

**ç»§æ‰¿**: `ReasoningVLA`

**æ–°å¢ç»„ä»¶**:

#### Expert Model
```python
# åŸºäº VLM çš„æ–‡æœ¬é…ç½®åˆ›å»ºä¸“å®¶æ¨¡å‹
expert_config = copy.deepcopy(self.vlm.config.text_config)
self.expert = AutoModel.from_config(expert_config)
```
- ç”¨äºå¤„ç†åŠ¨ä½œåºåˆ—çš„ Transformer æ¨¡å‹
- ä¸åŒ…å« `embed_tokens`ï¼ˆä½¿ç”¨ action_in_proj çš„è¾“å‡ºï¼‰

#### Action Space
```python
self.action_space: ActionSpace = hyu.instantiate(config.action_space_cfg)
```
- å®šä¹‰åŠ¨ä½œç©ºé—´ï¼ˆå¦‚åŠ é€Ÿåº¦-æ›²ç‡ç©ºé—´ï¼‰
- æä¾› `traj_to_action()` å’Œ `action_to_traj()` è½¬æ¢æ–¹æ³•

#### Diffusion Model
```python
self.diffusion: BaseDiffusion = hyu.instantiate(
    config.diffusion_cfg,
    x_dims=self.action_space.get_action_space_dims(),
)
```
- ç”¨äºåœ¨åŠ¨ä½œç©ºé—´ä¸­è¿›è¡Œå»å™ªé‡‡æ ·
- æ”¯æŒ Flow Matching ç­‰æ‰©æ•£æ–¹æ³•

#### Action Projection Layers

**Action Input Projection** (`action_in_proj`):
- å°†åŠ¨ä½œåºåˆ—æŠ•å½±åˆ°ä¸“å®¶æ¨¡å‹çš„éšè—ç©ºé—´
- ä½¿ç”¨ Fourier ç¼–ç å¤„ç†æ—¶é—´æ­¥ä¿¡æ¯
- è¾“å‡º: `(batch_size, num_waypoints, hidden_size)`

**Action Output Projection** (`action_out_proj`):
- å°†ä¸“å®¶æ¨¡å‹çš„è¾“å‡ºæŠ•å½±å›åŠ¨ä½œç©ºé—´
- è¾“å‡º: `(batch_size, num_waypoints, action_dim)`

### 2.3 æ•°æ®å¤„ç†æ¨¡å—

#### æ•°æ®åŠ è½½ (`load_physical_aiavdataset.py`)

**è¾“å…¥**:
- `clip_id`: æ•°æ®ç‰‡æ®µID
- `t0_us`: æ—¶é—´æˆ³ï¼ˆå¾®ç§’ï¼‰

**è¾“å‡ºå­—å…¸**:
```python
{
    "image_frames": (N_cameras, num_frames, 3, H, W),  # å¤šç›¸æœºå›¾åƒ
    "ego_history_xyz": (1, 1, num_history_steps, 3),   # å†å²ä½ç½®
    "ego_history_rot": (1, 1, num_history_steps, 3, 3), # å†å²æ—‹è½¬
    "ego_future_xyz": (1, 1, num_future_steps, 3),     # æœªæ¥ä½ç½®ï¼ˆground truthï¼‰
    "ego_future_rot": (1, 1, num_future_steps, 3, 3),  # æœªæ¥æ—‹è½¬ï¼ˆground truthï¼‰
    ...
}
```

**å…³é”®å¤„ç†**:
- åæ ‡è½¬æ¢ï¼šå°†ä¸–ç•Œåæ ‡ç³»è½¬æ¢ä¸º t0 æ—¶åˆ»çš„å±€éƒ¨åæ ‡ç³»
- æ—¶é—´é‡‡æ ·ï¼šå†å²è½¨è¿¹ 16 æ­¥ï¼ˆ1.6ç§’@10Hzï¼‰ï¼Œæœªæ¥è½¨è¿¹ 64 æ­¥ï¼ˆ6.4ç§’@10Hzï¼‰
- å›¾åƒåŠ è½½ï¼š4 ä¸ªç›¸æœºï¼Œæ¯ä¸ªç›¸æœº 4 å¸§

#### æ¶ˆæ¯æ„å»º (`helper.py`)

**`create_message()`** å‡½æ•°æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯ï¼š

```python
messages = [
    {
        "role": "system",
        "content": [{"type": "text", "text": "You are a driving assistant..."}]
    },
    {
        "role": "user",
        "content": [
            {"type": "image", "image": frame} for frame in frames  # å¤šç›¸æœºå›¾åƒ
        ] + [
            {
                "type": "text",
                "text": "<|traj_history_start|>...<|traj_history_end|>output the chain-of-thought..."
            }
        ]
    },
    {
        "role": "assistant",
        "content": [{"type": "text", "text": "<|cot_start|>"}]
    }
]
```

### 2.4 Processor

**`get_processor()`** å‡½æ•°ï¼š
- åŸºäº Qwen3-VL-2B-Instruct çš„ processor
- è®¾ç½®å›¾åƒåƒç´ èŒƒå›´ï¼š`min_pixels=163840`, `max_pixels=196608`
- ä½¿ç”¨è‡ªå®šä¹‰ tokenizerï¼ˆåŒ…å«è½¨è¿¹tokenï¼‰

---

## 3. æ¨ç†æµç¨‹è¯¦è§£

åŸºäº `test_inference.py` çš„å®Œæ•´æ¨ç†æµç¨‹ï¼š

### é˜¶æ®µ 1: æ•°æ®å‡†å¤‡

```python
# 1.1 åŠ è½½æ•°æ®é›†
clip_id = "030c760c-ae38-49aa-9ad8-f5650a545d26"
data = load_physical_aiavdataset(clip_id, t0_us=5_100_000)

# 1.2 æ„å»ºæ¶ˆæ¯
messages = helper.create_message(data["image_frames"].flatten(0, 1))
# messages åŒ…å«: system prompt + å¤šç›¸æœºå›¾åƒ + ç”¨æˆ·æŒ‡ä»¤
```

**æ•°æ®å½¢çŠ¶**:
- `image_frames`: `(N_cameras, num_frames, 3, H, W)` â†’ flatten ä¸º `(N_cameras*num_frames, 3, H, W)`
- `ego_history_xyz`: `(1, 1, 16, 3)`
- `ego_history_rot`: `(1, 1, 16, 3, 3)`

### é˜¶æ®µ 2: æ¨¡å‹åˆå§‹åŒ–

```python
# 2.1 åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = AlpamayoR1.from_pretrained(
    "nvidia/Alpamayo-R1-10B",
    dtype=torch.bfloat16
).to("cuda")

# 2.2 è·å– processor
processor = helper.get_processor(model.tokenizer)
```

**æ¨¡å‹ç»„ä»¶**:
- `model.vlm`: VLM backbone (Qwen3-VL)
- `model.expert`: Expert model
- `model.diffusion`: Diffusion model
- `model.action_space`: Action space
- `model.action_in_proj`: è¾“å…¥æŠ•å½±å±‚
- `model.action_out_proj`: è¾“å‡ºæŠ•å½±å±‚

### é˜¶æ®µ 3: è¾“å…¥å¤„ç†

```python
# 3.1 åº”ç”¨èŠå¤©æ¨¡æ¿å¹¶tokenize
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=False,
    continue_final_message=True,
    return_dict=True,
    return_tensors="pt",
)

# 3.2 èåˆå†å²è½¨è¿¹token
model_inputs = {
    "tokenized_data": inputs,
    "ego_history_xyz": data["ego_history_xyz"],
    "ego_history_rot": data["ego_history_rot"],
}
model_inputs = helper.to_device(model_inputs, "cuda")
```

**å…³é”®æ­¥éª¤**:
- `apply_chat_template()`: å°†æ¶ˆæ¯è½¬æ¢ä¸ºæ¨¡å‹è¾“å…¥æ ¼å¼
- `fuse_traj_tokens()`: å°†å†å²è½¨è¿¹ç¼–ç ä¸ºç¦»æ•£tokenå¹¶æ›¿æ¢å ä½ç¬¦

### é˜¶æ®µ 4: VLM è‡ªå›å½’ç”Ÿæˆ (Chain-of-Causation)

**ä½ç½®**: `sample_trajectories_from_data_with_vlm_rollout()` æ–¹æ³•

```python
# 4.1 èåˆè½¨è¿¹tokenåˆ°è¾“å…¥
input_ids = self.fuse_traj_tokens(input_ids, traj_data_vlm)

# 4.2 é…ç½®ç”Ÿæˆå‚æ•°
generation_config.top_p = 0.98
generation_config.temperature = 0.6
generation_config.num_return_sequences = num_traj_samples
generation_config.max_new_tokens = max_generation_length

# 4.3 ä½¿ç”¨ ExpertLogitsProcessor å±è”½è½¨è¿¹token
logits_processor = LogitsProcessorList([
    ExpertLogitsProcessor(
        traj_token_offset=self.config.traj_token_start_idx,
        traj_vocab_size=self.config.traj_vocab_size,
    )
])

# 4.4 ç”Ÿæˆ CoC æ¨ç†æ–‡æœ¬
vlm_outputs = self.vlm.generate(
    input_ids=input_ids,
    generation_config=generation_config,
    stopping_criteria=stopping_criteria,  # åœ¨ <traj_future_start> ååœæ­¢
    logits_processor=logits_processor,
    **tokenized_data,
)
```

**å…³é”®æœºåˆ¶**:
- **ExpertLogitsProcessor**: åœ¨ç”Ÿæˆ CoC æ—¶å±è”½ç¦»æ•£è½¨è¿¹tokenï¼Œç¡®ä¿åªç”Ÿæˆæ–‡æœ¬æ¨ç†
- **åœæ­¢æ¡ä»¶**: åœ¨é‡åˆ° `<|traj_future_start|>` token ååœæ­¢ç”Ÿæˆ
- **KV Cache**: ä¿å­˜ prompt çš„ key-value cache ä¾›åç»­ä½¿ç”¨

**è¾“å‡º**:
- `vlm_outputs.sequences`: ç”Ÿæˆçš„å®Œæ•´åºåˆ—ï¼ˆåŒ…å« CoC æ¨ç†ï¼‰
- `vlm_outputs.past_key_values`: KV cacheï¼ˆç”¨äºåç»­ä¸“å®¶æ¨¡å‹ï¼‰

### é˜¶æ®µ 5: Diffusion é‡‡æ ·è½¨è¿¹

#### 5.1 å‡†å¤‡ä¸“å®¶æ¨¡å‹è¾“å…¥

```python
# 5.1.1 æ‰¾åˆ° <traj_future_start> ä½ç½®
traj_future_start_positions = (vlm_outputs.sequences == eos_token_id).int().argmax(dim=1)

# 5.1.2 è®¾ç½®ä½ç½®IDå’Œæ³¨æ„åŠ›æ©ç 
position_ids = torch.arange(n_diffusion_tokens, device=device)
position_ids = einops.repeat(position_ids, "l -> 3 b l", b=b_star)
delta = vlm_outputs.rope_deltas + offset[:, None]
position_ids += delta.to(position_ids.device)

# 5.1.3 æ„å»ºæ³¨æ„åŠ›æ©ç ï¼ˆå±è”½ paddingï¼‰
attention_mask = torch.zeros(
    (b_star, 1, n_diffusion_tokens, prompt_cache.get_seq_length() + n_diffusion_tokens),
    ...
)
```

#### 5.2 å®šä¹‰å»å™ªæ­¥éª¤å‡½æ•°

```python
def step_fn(x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
    # x: (B*, *action_dim) - å™ªå£°åŠ¨ä½œ
    # t: æ—¶é—´æ­¥
    
    # 5.2.1 å°†å™ªå£°åŠ¨ä½œæŠ•å½±åˆ°ä¸“å®¶æ¨¡å‹çš„tokenåµŒå…¥
    future_token_embeds = self.action_in_proj(x, t)
    # å½¢çŠ¶: (b*, n_diffusion_tokens, hidden_size)
    
    # 5.2.2 è¿è¡Œä¸“å®¶æ¨¡å‹ï¼ˆä½¿ç”¨ KV cacheï¼‰
    expert_out_base = self.expert(
        inputs_embeds=future_token_embeds,
        position_ids=position_ids,
        past_key_values=prompt_cache,  # ä½¿ç”¨ VLM çš„ KV cache
        attention_mask=attention_mask,
        use_cache=True,
    )
    
    # 5.2.3 è£å‰ª KV cacheï¼ˆç§»é™¤æ–°æ·»åŠ çš„tokenï¼‰
    prompt_cache.crop(prefill_seq_len)
    
    # 5.2.4 æŠ•å½±å›åŠ¨ä½œç©ºé—´
    last_hidden = expert_out_base.last_hidden_state[:, -n_diffusion_tokens:]
    pred = self.action_out_proj(last_hidden)
    # å½¢çŠ¶: (b*, Tf, C_action) - é¢„æµ‹çš„å™ªå£°/å‘é‡åœº
    
    return pred
```

**å…³é”®è®¾è®¡**:
- **KV Cache å¤ç”¨**: ä¸“å®¶æ¨¡å‹å¤ç”¨ VLM ç”Ÿæˆçš„ KV cacheï¼Œå®ç°é«˜æ•ˆçš„ä¸Šä¸‹æ–‡ä¼ é€’
- **éå› æœæ³¨æ„åŠ›**: å¦‚æœé…ç½®äº† `expert_non_causal_attention=True`ï¼Œä¸“å®¶æ¨¡å‹å¯ä»¥ä½¿ç”¨åŒå‘æ³¨æ„åŠ›

#### 5.3 æ‰§è¡Œ Diffusion é‡‡æ ·

```python
# 5.3.1 é‡‡æ ·åŠ¨ä½œ
sampled_action = self.diffusion.sample(
    batch_size=total_batch,  # B * num_traj_samples * num_traj_sets
    step_fn=step_fn,
    device=device,
    return_all_steps=False,
    **diffusion_kwargs,
)

# 5.3.2 å°†åŠ¨ä½œè½¬æ¢ä¸ºè½¨è¿¹
pred_xyz, pred_rot = self.action_space.action_to_traj(
    sampled_action,
    hist_xyz_rep,  # é‡å¤çš„å†å²è½¨è¿¹
    hist_rot_rep,
)
```

**Diffusion è¿‡ç¨‹**:
- ä»å™ªå£°å¼€å§‹ï¼Œé€šè¿‡å¤šæ­¥å»å™ªç”ŸæˆåŠ¨ä½œåºåˆ—
- æ¯ä¸€æ­¥è°ƒç”¨ `step_fn` è¿›è¡Œå»å™ª
- æœ€ç»ˆå¾—åˆ°å¹²å‡€çš„åŠ¨ä½œåºåˆ—

### é˜¶æ®µ 6: åå¤„ç†ä¸è¾“å‡º

```python
# 6.1 é‡å¡‘è¾“å‡ºå½¢çŠ¶
pred_xyz = einops.rearrange(
    pred_xyz, "(b ns nj) ... -> b ns nj ...",
    ns=num_traj_sets, nj=num_traj_samples
)
# æœ€ç»ˆå½¢çŠ¶: (B, num_traj_sets, num_traj_samples, T, 3)

# 6.2 æå– CoC æ–‡æœ¬ï¼ˆå¦‚æœè¯·æ±‚ï¼‰
if kwargs.get("return_extra", False):
    extra = extract_text_tokens(self.tokenizer, vlm_outputs.sequences)
    # extra["cot"] åŒ…å«æ¯ä¸ªè½¨è¿¹çš„ Chain-of-Causation æ¨ç†æ–‡æœ¬
```

### é˜¶æ®µ 7: è¯„ä¼°æŒ‡æ ‡è®¡ç®—

```python
# 7.1 è®¡ç®— minADE
gt_xy = data["ego_future_xyz"].cpu()[0, 0, :, :2].T.numpy()
pred_xy = pred_xyz.cpu().numpy()[0, 0, :, :, :2].transpose(0, 2, 1)
diff = np.linalg.norm(pred_xy - gt_xy[None, ...], axis=1).mean(-1)
min_ade = diff.min()  # æœ€å°å¹³å‡ä½ç§»è¯¯å·®
```

---

## 4. æ•°æ®æµä¸å¼ é‡å½¢çŠ¶

### å®Œæ•´æ•°æ®æµ

```
è¾“å…¥æ•°æ®
  â”œâ”€â”€ image_frames: (4, 4, 3, H, W)
  â”œâ”€â”€ ego_history_xyz: (1, 1, 16, 3)
  â””â”€â”€ ego_history_rot: (1, 1, 16, 3, 3)
         â†“
æ¶ˆæ¯æ„å»º (create_message)
  â””â”€â”€ messages: List[Dict] (å¤šæ¨¡æ€æ¶ˆæ¯)
         â†“
Processor (apply_chat_template)
  â””â”€â”€ tokenized_data: Dict
      â”œâ”€â”€ input_ids: (1, L_prompt)
      â””â”€â”€ attention_mask: (1, L_prompt)
         â†“
è½¨è¿¹tokenèåˆ (fuse_traj_tokens)
  â””â”€â”€ input_ids: (1, L_prompt) [å†å²è½¨è¿¹tokenå·²æ›¿æ¢]
         â†“
VLM ç”Ÿæˆ (generate)
  â”œâ”€â”€ vlm_outputs.sequences: (num_traj_samples, L_total)
  â”œâ”€â”€ vlm_outputs.past_key_values: KV Cache
  â””â”€â”€ vlm_outputs.rope_deltas: RoPE åç§»
         â†“
Diffusion é‡‡æ ·å‡†å¤‡
  â”œâ”€â”€ position_ids: (3, num_traj_samples, n_diffusion_tokens)
  â””â”€â”€ attention_mask: (num_traj_samples, 1, n_diffusion_tokens, L_total)
         â†“
Diffusion é‡‡æ ·å¾ªç¯ (step_fn)
  â”œâ”€â”€ x (å™ªå£°åŠ¨ä½œ): (B*, n_diffusion_tokens, action_dim)
  â”œâ”€â”€ action_in_proj â†’ future_token_embeds: (B*, n_diffusion_tokens, hidden_size)
  â”œâ”€â”€ expert â†’ last_hidden: (B*, n_diffusion_tokens, hidden_size)
  â””â”€â”€ action_out_proj â†’ pred: (B*, n_diffusion_tokens, action_dim)
         â†“
åŠ¨ä½œåˆ°è½¨è¿¹è½¬æ¢ (action_to_traj)
  â”œâ”€â”€ pred_xyz: (B*, n_diffusion_tokens, 3)
  â””â”€â”€ pred_rot: (B*, n_diffusion_tokens, 3, 3)
         â†“
è¾“å‡ºé‡å¡‘
  â”œâ”€â”€ pred_xyz: (1, num_traj_sets, num_traj_samples, 64, 3)
  â””â”€â”€ pred_rot: (1, num_traj_sets, num_traj_samples, 64, 3, 3)
```

### å…³é”®å¼ é‡å½¢çŠ¶æ€»ç»“

| é˜¶æ®µ | å¼ é‡åç§° | å½¢çŠ¶ | è¯´æ˜ |
|------|---------|------|------|
| è¾“å…¥ | `image_frames` | `(4, 4, 3, H, W)` | 4ä¸ªç›¸æœºï¼Œæ¯ç›¸æœº4å¸§ |
| è¾“å…¥ | `ego_history_xyz` | `(1, 1, 16, 3)` | å†å²16æ­¥ä½ç½® |
| VLMè¾“å…¥ | `input_ids` | `(1, L_prompt)` | Tokenizedè¾“å…¥åºåˆ— |
| VLMè¾“å‡º | `vlm_outputs.sequences` | `(num_traj_samples, L_total)` | ç”Ÿæˆçš„å®Œæ•´åºåˆ— |
| Diffusion | `x` (å™ªå£°åŠ¨ä½œ) | `(B*, n_diffusion_tokens, action_dim)` | åŠ¨ä½œç©ºé—´ä¸­çš„å™ªå£° |
| Expertè¾“å…¥ | `future_token_embeds` | `(B*, n_diffusion_tokens, hidden_size)` | æŠ•å½±åçš„tokenåµŒå…¥ |
| Expertè¾“å‡º | `last_hidden` | `(B*, n_diffusion_tokens, hidden_size)` | ä¸“å®¶æ¨¡å‹éšè—çŠ¶æ€ |
| æœ€ç»ˆè¾“å‡º | `pred_xyz` | `(1, num_traj_sets, num_traj_samples, 64, 3)` | é¢„æµ‹è½¨è¿¹ä½ç½® |
| æœ€ç»ˆè¾“å‡º | `pred_rot` | `(1, num_traj_sets, num_traj_samples, 64, 3, 3)` | é¢„æµ‹è½¨è¿¹æ—‹è½¬ |

**ç¬¦å·è¯´æ˜**:
- `B*`: æ‰¹æ¬¡å¤§å° Ã— è½¨è¿¹æ ·æœ¬æ•°
- `n_diffusion_tokens`: æ‰©æ•£tokenæ•°é‡ï¼ˆé€šå¸¸ç­‰äºåŠ¨ä½œç©ºé—´çš„æ—¶é—´æ­¥æ•°ï¼Œå¦‚64ï¼‰
- `L_prompt`: Prompté•¿åº¦
- `L_total`: æ€»åºåˆ—é•¿åº¦ï¼ˆprompt + ç”Ÿæˆéƒ¨åˆ†ï¼‰

---

# ç¬¬äºŒéƒ¨åˆ†ï¼šExpert Model è¯¦è§£

## 5. ä»€ä¹ˆæ˜¯ Expert Modelï¼Ÿ

### åŸºæœ¬å®šä¹‰

**Expert Model** æ˜¯ Alpamayo-R1 ä¸­çš„ä¸€ä¸ª**ä¸“é—¨çš„ Transformer æ¨¡å‹**ï¼Œç”¨äºå¤„ç†**åŠ¨ä½œåºåˆ—**ï¼ˆaction sequencesï¼‰ã€‚

### åœ¨ä»£ç ä¸­çš„ä½ç½®

```python
# alpamayo_r1.py ç¬¬ 73-74 è¡Œ
class AlpamayoR1(ReasoningVLA):
    """Expert model for reasoning VLA."""
```

### æ ¸å¿ƒç‰¹ç‚¹

- **ä¸“é—¨å¤„ç†åŠ¨ä½œ**ï¼šä¸åŒäº VLM å¤„ç†æ–‡æœ¬å’Œå›¾åƒï¼ŒExpert Model ä¸“é—¨å¤„ç†åŠ¨ä½œåºåˆ—
- **åŸºäº VLM æ¶æ„**ï¼šä½¿ç”¨ä¸ VLM ç›¸åŒçš„æ–‡æœ¬æ¨¡å‹æ¶æ„ï¼ˆtext_configï¼‰
- **éå› æœ Attention**ï¼šä½¿ç”¨ non-causal attentionï¼Œå…è®¸æ‰€æœ‰æ—¶é—´æ­¥ä¹‹é—´äº’ç›¸ attention
- **ä¸ Diffusion é…åˆ**ï¼šåœ¨ diffusion é‡‡æ ·è¿‡ç¨‹ä¸­ï¼Œç”¨äºé¢„æµ‹å»å™ªæ–¹å‘

---

## 6. Expert Model çš„æ¶æ„

### åˆ›å»ºè¿‡ç¨‹

```python
# alpamayo_r1.py ç¬¬ 87-94 è¡Œ
# 1. å¤åˆ¶ VLM çš„æ–‡æœ¬é…ç½®
expert_config = copy.deepcopy(self.vlm.config.text_config)

# 2. åº”ç”¨è‡ªå®šä¹‰é…ç½®ï¼ˆå¦‚æœæœ‰ï¼‰
if config.expert_cfg is not None:
    for key, value in config.expert_cfg.items():
        setattr(expert_config, key, value)

# 3. åˆ›å»º Expert Model
self.expert = AutoModel.from_config(expert_config)

# 4. åˆ é™¤ embed_tokensï¼ˆå› ä¸ºä½¿ç”¨ action_in_proj çš„è¾“å‡ºï¼‰
del self.expert.embed_tokens
```

### å…³é”®ç»„ä»¶

Expert Model ä¸ä»¥ä¸‹ç»„ä»¶é…åˆå·¥ä½œï¼š

```python
# è¾“å…¥æŠ•å½±ï¼šå°†åŠ¨ä½œåºåˆ—æŠ•å½±åˆ° Expert Model çš„éšè—ç©ºé—´
self.action_in_proj = hyu.instantiate(
    config.action_in_proj_cfg,
    in_dims=self.action_space.get_action_space_dims(),
    out_dim=expert_config.hidden_size,  # è¾“å‡ºç»´åº¦åŒ¹é… Expert Model
)

# Expert Model æœ¬èº«
self.expert = AutoModel.from_config(expert_config)

# è¾“å‡ºæŠ•å½±ï¼šå°† Expert Model çš„è¾“å‡ºæŠ•å½±å›åŠ¨ä½œç©ºé—´
self.action_out_proj = hyu.instantiate(
    config.action_out_proj_cfg,
    in_features=expert_config.hidden_size,
    out_features=self.action_space.get_action_space_dims()[-1],
)
```

### æ¶æ„ç¤ºæ„å›¾

```
è¾“å…¥ï¼šå™ªå£°åŠ¨ä½œåºåˆ— (x, t)
    â†“
Action Input Projection (action_in_proj)
    â†“
[åŠ¨ä½œåºåˆ— embeddings] (future_token_embeds)
    â†“
Expert Model (Transformer)
    â”œâ”€â”€ è¾“å…¥ï¼šfuture_token_embeds
    â”œâ”€â”€ KV Cacheï¼šprompt_cache (æ¥è‡ª VLM)
    â”œâ”€â”€ Attentionï¼šNon-Causal (åŒå‘)
    â””â”€â”€ è¾“å‡ºï¼šhidden states
    â†“
Action Output Projection (action_out_proj)
    â†“
è¾“å‡ºï¼šé¢„æµ‹çš„å™ªå£°/å‘é‡åœº (pred)
```

---

## 7. Expert Model çš„ä½œç”¨

### åœ¨ Diffusion é‡‡æ ·ä¸­çš„ä½œç”¨

Expert Model æ˜¯ **diffusion é‡‡æ ·è¿‡ç¨‹ä¸­çš„å»å™ªå‡½æ•°**ï¼ˆdenoising functionï¼‰ï¼š

```python
# alpamayo_r1.py ç¬¬ 255-284 è¡Œ
def step_fn(x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
    """Diffusion çš„å»å™ªæ­¥éª¤å‡½æ•°"""
    # x: å™ªå£°åŠ¨ä½œåºåˆ— (B*, 64, action_dim)
    # t: æ—¶é—´æ­¥
    
    # 1. å°†åŠ¨ä½œåºåˆ—æŠ•å½±ä¸º token embeddings
    future_token_embeds = self.action_in_proj(x, t)
    # å½¢çŠ¶: (B*, 64, hidden_size)
    
    # 2. Expert Model å¤„ç†
    expert_out_base = self.expert(
        inputs_embeds=future_token_embeds,
        past_key_values=prompt_cache,  # å¤ç”¨ VLM çš„ KV cache
        attention_mask=attention_mask,
        is_causal=False,  # éå› æœ attention
    )
    
    # 3. æå– hidden states
    last_hidden = expert_out_base.last_hidden_state[:, -n_diffusion_tokens:]
    
    # 4. æŠ•å½±å›åŠ¨ä½œç©ºé—´
    pred = self.action_out_proj(last_hidden)
    # å½¢çŠ¶: (B*, 64, action_dim) - é¢„æµ‹çš„å™ªå£°/å‘é‡åœº
    
    return pred
```

### å…³é”®åŠŸèƒ½

#### åŠŸèƒ½ 1: ä¸Šä¸‹æ–‡ç†è§£
- **è¾“å…¥**ï¼šåŠ¨ä½œåºåˆ—çš„ embeddings + VLM çš„ KV cache
- **ä½œç”¨**ï¼šç†è§£è§†è§‰ä¸Šä¸‹æ–‡ï¼ˆå›¾åƒï¼‰ã€å†å²è½¨è¿¹ã€CoC æ¨ç†æ–‡æœ¬
- **è¾“å‡º**ï¼šåŸºäºä¸Šä¸‹æ–‡ç†è§£çš„åŠ¨ä½œé¢„æµ‹

#### åŠŸèƒ½ 2: æ—¶åºå»ºæ¨¡
- **è¾“å…¥**ï¼šæ‰€æœ‰æ—¶é—´æ­¥çš„åŠ¨ä½œ embeddingsï¼ˆå¹¶è¡Œï¼‰
- **ä½œç”¨**ï¼šä½¿ç”¨ non-causal attention å»ºç«‹æ—¶é—´æ­¥ä¹‹é—´çš„å…³ç³»
- **è¾“å‡º**ï¼šå…¨å±€ä¸€è‡´çš„åŠ¨ä½œåºåˆ—

#### åŠŸèƒ½ 3: å»å™ªé¢„æµ‹
- **è¾“å…¥**ï¼šå™ªå£°åŠ¨ä½œåºåˆ— + æ—¶é—´æ­¥
- **ä½œç”¨**ï¼šé¢„æµ‹å»å™ªæ–¹å‘ï¼ˆå™ªå£°æˆ–å‘é‡åœºï¼‰
- **è¾“å‡º**ï¼šç”¨äº diffusion ä¸‹ä¸€æ­¥çš„é¢„æµ‹

### ä¸ VLM çš„åˆ†å·¥

| ç»„ä»¶ | ä½œç”¨ | è¾“å…¥ | è¾“å‡º |
|------|------|------|------|
| **VLM** | ç”Ÿæˆ CoC æ¨ç†æ–‡æœ¬ | å›¾åƒã€å†å²è½¨è¿¹ã€ç”¨æˆ·æŒ‡ä»¤ | CoC æ–‡æœ¬ tokens + KV cache |
| **Expert Model** | å¤„ç†åŠ¨ä½œåºåˆ— | åŠ¨ä½œ embeddings + KV cache | åŠ¨ä½œé¢„æµ‹ï¼ˆå»å™ªæ–¹å‘ï¼‰ |

**å…³é”®åŒºåˆ«**ï¼š
- **VLM**ï¼šå¤„ç†**æ–‡æœ¬ç”Ÿæˆ**ï¼ˆè‡ªå›å½’ï¼Œcausal attentionï¼‰
- **Expert Model**ï¼šå¤„ç†**åŠ¨ä½œåºåˆ—**ï¼ˆå¹¶è¡Œï¼Œnon-causal attentionï¼‰

---

## 8. ä¸ VLM çš„å…³ç³»

### æ¶æ„ç›¸ä¼¼æ€§

```python
# Expert Model ä½¿ç”¨ä¸ VLM ç›¸åŒçš„æ–‡æœ¬æ¨¡å‹æ¶æ„
expert_config = copy.deepcopy(self.vlm.config.text_config)
self.expert = AutoModel.from_config(expert_config)
```

**ç›¸åŒç‚¹**ï¼š
- ç›¸åŒçš„ Transformer æ¶æ„ï¼ˆå±‚æ•°ã€éšè—ç»´åº¦ã€æ³¨æ„åŠ›å¤´æ•°ç­‰ï¼‰
- ç›¸åŒçš„å‚æ•°ç»“æ„

**ä¸åŒç‚¹**ï¼š
- **VLM**ï¼šåŒ…å«è§†è§‰ç¼–ç å™¨ + æ–‡æœ¬æ¨¡å‹
- **Expert Model**ï¼šåªæœ‰æ–‡æœ¬æ¨¡å‹éƒ¨åˆ†ï¼ˆæ²¡æœ‰è§†è§‰ç¼–ç å™¨ï¼‰
- **Expert Model**ï¼šæ²¡æœ‰ `embed_tokens`ï¼ˆä½¿ç”¨ `action_in_proj` çš„è¾“å‡ºï¼‰

### KV Cache å¤ç”¨

**å…³é”®è®¾è®¡**ï¼šExpert Model **å¤ç”¨ VLM ç”Ÿæˆçš„ KV cache**

```python
# VLM ç”Ÿæˆ CoC å
prompt_cache = vlm_outputs.past_key_values  # åŒ…å«æ‰€æœ‰ prompt çš„ KV

# Expert Model ä½¿ç”¨è¿™ä¸ª cache
expert_out_base = self.expert(
    inputs_embeds=future_token_embeds,
    past_key_values=prompt_cache,  # â­ å¤ç”¨ VLM çš„ KV cache
    ...
)
```

**å¥½å¤„**ï¼š
- **æ•ˆç‡**ï¼šé¿å…é‡å¤è®¡ç®— prompt çš„ KV cache
- **ä¸€è‡´æ€§**ï¼šExpert Model å’Œ VLM çœ‹åˆ°ç›¸åŒçš„ä¸Šä¸‹æ–‡
- **å†…å­˜**ï¼šå…±äº« KV cacheï¼ŒèŠ‚çœå†…å­˜

### ä¸Šä¸‹æ–‡ä¼ é€’

Expert Model é€šè¿‡ KV cache å¯ä»¥è®¿é—®ï¼š
- **å›¾åƒ tokens**ï¼ˆé€šè¿‡ VLM çš„è§†è§‰ç¼–ç å™¨ï¼‰
- **å†å²è½¨è¿¹ tokens**
- **CoC æ¨ç†æ–‡æœ¬ tokens**
- **ç”¨æˆ·æŒ‡ä»¤ tokens**

è¿™ä½¿å¾— Expert Model èƒ½å¤ŸåŸºäº**å®Œæ•´çš„ä¸Šä¸‹æ–‡**æ¥é¢„æµ‹åŠ¨ä½œã€‚

---

# ç¬¬ä¸‰éƒ¨åˆ†ï¼šAttention æœºåˆ¶åˆ†æ

## 9. Causal vs Non-Causal Attention

åœ¨ Alpamayo ä¸­ï¼Œ**ä¸åŒçš„æ¨¡å‹ç»„ä»¶ä½¿ç”¨ä¸åŒçš„ attention ç±»å‹**ï¼š

| æ¨¡å‹ç»„ä»¶ | Attention ç±»å‹ | åŸå›  |
|---------|---------------|------|
| **VLM (ç”Ÿæˆ CoC)** | **Causal (å› æœ)** | è‡ªå›å½’ç”Ÿæˆï¼Œéœ€è¦å› æœæ©ç  |
| **Expert Model (å¤„ç†åŠ¨ä½œ)** | **Non-Causal (éå› æœ)** | å¹¶è¡Œå¤„ç†æ‰€æœ‰æœªæ¥æ—¶é—´æ­¥ |

---

## 10. VLM çš„ Causal Attention

### ä»£ç ä½ç½®

```python
# alpamayo_r1.py ç¬¬ 192-198 è¡Œ
vlm_outputs = self.vlm.generate(
    input_ids=input_ids,
    generation_config=generation_config,
    stopping_criteria=stopping_criteria,
    logits_processor=logits_processor,
    **tokenized_data,
)
```

### ä¸ºä»€ä¹ˆä½¿ç”¨ Causal Attentionï¼Ÿ

**VLM ç”Ÿæˆ Chain-of-Causation (CoC) æ–‡æœ¬æ—¶**ï¼š
- ä½¿ç”¨æ ‡å‡†çš„**è‡ªå›å½’ç”Ÿæˆ**ï¼ˆautoregressive generationï¼‰
- æ¯ä¸ª token åªèƒ½çœ‹åˆ°**ä¹‹å‰çš„ tokens**
- è¿™æ˜¯ LLM çš„æ ‡å‡†è¡Œä¸º

**Causal Mask ç¤ºä¾‹**ï¼š
```
Token 0:  [1, 0, 0, 0, 0]  â† åªèƒ½çœ‹åˆ°è‡ªå·±
Token 1:  [1, 1, 0, 0, 0]  â† å¯ä»¥çœ‹åˆ° Token 0 å’Œè‡ªå·±
Token 2:  [1, 1, 1, 0, 0]  â† å¯ä»¥çœ‹åˆ° Token 0, 1 å’Œè‡ªå·±
Token 3:  [1, 1, 1, 1, 0]  â† å¯ä»¥çœ‹åˆ° Token 0, 1, 2 å’Œè‡ªå·±
Token 4:  [1, 1, 1, 1, 1]  â† å¯ä»¥çœ‹åˆ°æ‰€æœ‰ä¹‹å‰çš„ tokens
```

**åŸå› **ï¼š
- ç”Ÿæˆè¿‡ç¨‹æ˜¯**é¡ºåºçš„**ï¼šå…ˆç”Ÿæˆ Token 0ï¼Œå†ç”Ÿæˆ Token 1ï¼Œ...
- å¿…é¡»ä½¿ç”¨ causal mask æ‰èƒ½ä¿è¯ç”Ÿæˆçš„ä¸€è‡´æ€§
- è¿™æ˜¯æ‰€æœ‰è‡ªå›å½’è¯­è¨€æ¨¡å‹çš„æ ‡å‡†åšæ³•

### VLM çš„ Attention èŒƒå›´

åœ¨ç”Ÿæˆ CoC æ—¶ï¼š
- **Prompt éƒ¨åˆ†**ï¼ˆå›¾åƒã€å†å²è½¨è¿¹ã€ç”¨æˆ·æŒ‡ä»¤ï¼‰ï¼šæ‰€æœ‰ tokens å¯è§
- **ç”Ÿæˆéƒ¨åˆ†**ï¼ˆCoC æ–‡æœ¬ï¼‰ï¼šä½¿ç”¨ causal maskï¼Œåªèƒ½çœ‹åˆ°ä¹‹å‰çš„ tokens

---

## 11. Expert Model çš„ Non-Causal Attention

### ä»£ç ä½ç½®

```python
# config.py ç¬¬ 36 è¡Œ
expert_non_causal_attention: bool = True,  # é»˜è®¤å€¼

# alpamayo_r1.py ç¬¬ 250-252 è¡Œ
forward_kwargs = {}
if self.config.expert_non_causal_attention:
    forward_kwargs["is_causal"] = False  # â­ å…³é”®ï¼šè®¾ç½®ä¸ºéå› æœ

# alpamayo_r1.py ç¬¬ 269-276 è¡Œ
expert_out_base = self.expert(
    inputs_embeds=future_token_embeds,  # æ‰€æœ‰æœªæ¥æ—¶é—´æ­¥çš„ token embeddings
    position_ids=position_ids,
    past_key_values=prompt_cache,  # åŒ…å« prompt å’Œ CoC çš„ KV cache
    attention_mask=attention_mask,
    use_cache=True,
    **forward_kwargs,  # is_causal=False
)
```

### ä¸ºä»€ä¹ˆä½¿ç”¨ Non-Causal Attentionï¼Ÿ

**Expert Model å¤„ç†åŠ¨ä½œåºåˆ—æ—¶**ï¼š
- è¾“å…¥æ˜¯**æ‰€æœ‰æœªæ¥æ—¶é—´æ­¥çš„ token embeddings**ï¼ˆä¾‹å¦‚ 64 ä¸ªæ—¶é—´æ­¥ï¼‰
- è¿™äº› embeddings æ˜¯**å¹¶è¡Œè¾“å…¥**çš„ï¼ˆä¸æ˜¯é¡ºåºç”Ÿæˆï¼‰
- ä½¿ç”¨ **non-causal attention** å…è®¸æ‰€æœ‰æ—¶é—´æ­¥ä¹‹é—´äº’ç›¸ attention

**Non-Causal Mask ç¤ºä¾‹**ï¼ˆå‡è®¾ 5 ä¸ªæ—¶é—´æ­¥ï¼‰ï¼š
```
Time 0:  [1, 1, 1, 1, 1]  â† å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ—¶é—´æ­¥
Time 1:  [1, 1, 1, 1, 1]  â† å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ—¶é—´æ­¥
Time 2:  [1, 1, 1, 1, 1]  â† å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ—¶é—´æ­¥
Time 3:  [1, 1, 1, 1, 1]  â† å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ—¶é—´æ­¥
Time 4:  [1, 1, 1, 1, 1]  â† å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ—¶é—´æ­¥
```

**å…³é”®åŒºåˆ«**ï¼š
- **Causal**: æ—¶é—´æ­¥ i åªèƒ½çœ‹åˆ°æ—¶é—´æ­¥ 0 åˆ° i
- **Non-Causal**: æ—¶é—´æ­¥ i å¯ä»¥çœ‹åˆ°æ‰€æœ‰æ—¶é—´æ­¥ï¼ˆ0 åˆ° T-1ï¼‰

### ä¸ºä»€ä¹ˆ Expert Model ä½¿ç”¨ Non-Causalï¼Ÿ

#### 1. å¹¶è¡Œå¤„ç†éœ€æ±‚

**Diffusion é‡‡æ ·è¿‡ç¨‹**ï¼š
```python
# åœ¨ diffusion çš„æ¯ä¸€æ­¥ï¼Œæ‰€æœ‰æ—¶é—´æ­¥çš„åŠ¨ä½œéƒ½æ˜¯å¹¶è¡Œå¤„ç†çš„
def step_fn(x: torch.Tensor, t: torch.Tensor) -> torch.Tensor:
    # x: (B*, 64, action_dim) - æ‰€æœ‰ 64 ä¸ªæ—¶é—´æ­¥çš„å™ªå£°åŠ¨ä½œ
    future_token_embeds = self.action_in_proj(x, t)  # å¹¶è¡ŒæŠ•å½±æ‰€æœ‰æ—¶é—´æ­¥
    expert_out = self.expert(
        inputs_embeds=future_token_embeds,  # å¹¶è¡Œè¾“å…¥æ‰€æœ‰æ—¶é—´æ­¥
        ...
    )
    return pred  # å¹¶è¡Œè¾“å‡ºæ‰€æœ‰æ—¶é—´æ­¥çš„é¢„æµ‹
```

**å…³é”®ç‚¹**ï¼š
- æ‰€æœ‰æ—¶é—´æ­¥çš„ embeddings æ˜¯**åŒæ—¶è¾“å…¥**çš„
- ä¸æ˜¯é¡ºåºç”Ÿæˆï¼Œè€Œæ˜¯**å¹¶è¡Œå¤„ç†**
- å› æ­¤å¯ä»¥ä½¿ç”¨ non-causal attention

#### 2. æ—¶åºä¾èµ–å»ºæ¨¡

**Non-Causal Attention çš„ä¼˜åŠ¿**ï¼š
- **åŒå‘ä¿¡æ¯æµ**ï¼šæ¯ä¸ªæ—¶é—´æ­¥å¯ä»¥çœ‹åˆ°è¿‡å»å’Œæœªæ¥çš„ä¿¡æ¯
- **å…¨å±€ä¸€è‡´æ€§**ï¼šæ‰€æœ‰æ—¶é—´æ­¥å¯ä»¥åè°ƒä¸€è‡´ï¼Œé¿å…å±€éƒ¨ä¸ä¸€è‡´
- **æ›´å¥½çš„è½¨è¿¹å¹³æ»‘æ€§**ï¼šæœªæ¥æ—¶é—´æ­¥å¯ä»¥å½±å“è¿‡å»æ—¶é—´æ­¥çš„é¢„æµ‹

#### 3. ä¸ Diffusion çš„é…åˆ

**Diffusion æ¨¡å‹çš„ç‰¹ç‚¹**ï¼š
- åœ¨å»å™ªè¿‡ç¨‹ä¸­ï¼Œæ‰€æœ‰æ—¶é—´æ­¥æ˜¯**åŒæ—¶ä¼˜åŒ–**çš„
- éœ€è¦å…¨å±€ä¸€è‡´æ€§ï¼Œè€Œä¸æ˜¯å±€éƒ¨å› æœæ€§
- Non-causal attention æ›´é€‚åˆè¿™ç§å¹¶è¡Œä¼˜åŒ–è¿‡ç¨‹

---

# ç¬¬å››éƒ¨åˆ†ï¼šVision Token Attention

## 12. å›¾åƒæ•°æ®ç»„ç»‡

### åŸå§‹æ•°æ®ç»“æ„

ä» `load_physical_aiavdataset.py` ä¸­å¯ä»¥çœ‹åˆ°ï¼š

```python
# é»˜è®¤åŠ è½½ 4 ä¸ªç›¸æœºï¼Œæ¯ä¸ªç›¸æœº 4 å¸§
camera_features = [
    CAMERA_CROSS_LEFT_120FOV,      # ç›¸æœº 0
    CAMERA_FRONT_WIDE_120FOV,      # ç›¸æœº 1
    CAMERA_CROSS_RIGHT_120FOV,     # ç›¸æœº 2
    CAMERA_FRONT_TELE_30FOV,       # ç›¸æœº 3
]

# å›¾åƒå½¢çŠ¶
image_frames: (N_cameras=4, num_frames=4, 3, H, W)
```

### æ—¶é—´å¸§é¡ºåº

```python
# ç¬¬ 161-165 è¡Œï¼šå›¾åƒæ—¶é—´æˆ³
# å¦‚æœ num_frames=4ï¼ŒåŠ è½½æ—¶é—´ç‚¹ï¼š[t0-0.3s, t0-0.2s, t0-0.1s, t0]
image_timestamps = np.array(
    [t0_us - (num_frames - 1 - i) * int(time_step * 1_000_000) 
     for i in range(num_frames)],
    dtype=np.int64,
)
```

**æ—¶é—´é¡ºåº**ï¼ˆä»æ—©åˆ°æ™šï¼‰ï¼š
- Frame 0: t0 - 0.3s
- Frame 1: t0 - 0.2s
- Frame 2: t0 - 0.1s
- Frame 3: t0 (å½“å‰æ—¶åˆ»)

### å›¾åƒå±•å¹³é¡ºåº

åœ¨ `test_inference.py` ç¬¬ 33 è¡Œï¼š

```python
messages = helper.create_message(data["image_frames"].flatten(0, 1))
```

**`flatten(0, 1)` çš„æ•ˆæœ**ï¼š
- è¾“å…¥ï¼š`(4, 4, 3, H, W)` - (N_cameras, num_frames, C, H, W)
- è¾“å‡ºï¼š`(16, 3, H, W)` - (N_cameras * num_frames, C, H, W)

**å±•å¹³åçš„é¡ºåº**ï¼ˆ**å…ˆæŒ‰ç›¸æœºï¼Œå†æŒ‰å¸§**ï¼‰ï¼š

```
Index 0:  Camera 0, Frame 0 (t0-0.3s) - cross_left, æœ€æ—©
Index 1:  Camera 0, Frame 1 (t0-0.2s) - cross_left
Index 2:  Camera 0, Frame 2 (t0-0.1s) - cross_left
Index 3:  Camera 0, Frame 3 (t0)      - cross_left, å½“å‰
Index 4:  Camera 1, Frame 0 (t0-0.3s) - front_wide, æœ€æ—©
Index 5:  Camera 1, Frame 1 (t0-0.2s) - front_wide
Index 6:  Camera 1, Frame 2 (t0-0.1s) - front_wide
Index 7:  Camera 1, Frame 3 (t0)      - front_wide, å½“å‰
Index 8:  Camera 2, Frame 0 (t0-0.3s) - cross_right, æœ€æ—©
Index 9:  Camera 2, Frame 1 (t0-0.2s) - cross_right
Index 10: Camera 2, Frame 2 (t0-0.1s) - cross_right
Index 11: Camera 2, Frame 3 (t0)      - cross_right, å½“å‰
Index 12: Camera 6, Frame 0 (t0-0.3s) - front_tele, æœ€æ—©
Index 13: Camera 6, Frame 1 (t0-0.2s) - front_tele
Index 14: Camera 6, Frame 2 (t0-0.1s) - front_tele
Index 15: Camera 6, Frame 3 (t0)      - front_tele, å½“å‰
```

**å…³é”®ç‰¹ç‚¹**ï¼š
- **View-first ordering**ï¼šå…ˆæŒ‰ç›¸æœºï¼ˆviewï¼‰åˆ†ç»„ï¼Œå†æŒ‰æ—¶é—´ï¼ˆframeï¼‰æ’åº
- æ¯ä¸ªç›¸æœºçš„ 4 å¸§æŒ‰æ—¶é—´é¡ºåºæ’åˆ—ï¼ˆä»æ—©åˆ°æ™šï¼‰
- æ‰€æœ‰ç›¸æœºçš„åŒä¸€æ—¶é—´å¸§**ä¸è¿ç»­**ï¼ˆä¾‹å¦‚ï¼Œæ‰€æœ‰ Frame 0 åˆ†æ•£åœ¨ä¸åŒä½ç½®ï¼‰

---

## 13. Vision Token ç”Ÿæˆ

### Message æ„å»º

åœ¨ `helper.py` çš„ `create_message()` å‡½æ•°ä¸­ï¼š

```python
def create_message(frames: torch.Tensor):
    """frames: (16, 3, H, W) after flatten(0, 1)"""
    return [
        {
            "role": "system",
            "content": [{"type": "text", "text": "You are a driving assistant..."}]
        },
        {
            "role": "user",
            "content": [
                {"type": "image", "image": frame} for frame in frames  # 16 å¼ å›¾åƒ
            ] + [
                {"type": "text", "text": "<|traj_history_start|>...<|traj_history_end|>..."}
            ]
        },
        {
            "role": "assistant",
            "content": [{"type": "text", "text": "<|cot_start|>"}]
        },
    ]
```

### Processor å¤„ç†

åœ¨ `test_inference.py` ä¸­ï¼š

```python
processor = helper.get_processor(model.tokenizer)  # Qwen3-VL processor

inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    add_generation_prompt=False,
    continue_final_message=True,
    return_dict=True,
    return_tensors="pt",
)
```

**Qwen3-VL Processor çš„å¤„ç†æµç¨‹**ï¼š
1. å¯¹æ¯å¼ å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼ˆresize, normalizeï¼‰
2. é€šè¿‡ Vision Encoder å°†å›¾åƒç¼–ç ä¸º vision tokens
3. å°† vision tokens æ’å…¥åˆ°æ–‡æœ¬ token åºåˆ—ä¸­
4. æ„å»º attention mask

---

## 14. Vision Token Attention è§„åˆ™

### å…³é”®ç†è§£ï¼šPrompt vs Generation

åœ¨ VLM ç”Ÿæˆæ—¶ï¼Œåºåˆ—åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼š

```
[Prompt Tokens] [Generation Tokens]
     â†‘                â†‘
  å…¨è¿æ¥          Causal Mask
```

- **Prompt Tokens**ï¼šåŒ…æ‹¬ vision tokensã€å†å²è½¨è¿¹ tokensã€ç”¨æˆ·æŒ‡ä»¤ tokens
- **Generation Tokens**ï¼šCoC æ–‡æœ¬ tokensï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰

### Vision Token çš„ Attention

**Vision tokens éƒ½åœ¨ Prompt ä¸­**ï¼Œå› æ­¤ï¼š

#### è§„åˆ™ 1: Vision Tokens ä¹‹é—´å…¨è¿æ¥

```
Vision Token 0 (Camera 0, Frame 0)  â†â†’  Vision Token 1 (Camera 0, Frame 1)
Vision Token 0 (Camera 0, Frame 0)  â†â†’  Vision Token 4 (Camera 1, Frame 0)
Vision Token 0 (Camera 0, Frame 0)  â†â†’  Vision Token 15 (Camera 6, Frame 3)
... (æ‰€æœ‰ vision tokens ä¹‹é—´éƒ½å¯ä»¥ attention)
```

**åŸå› **ï¼š
- Vision tokens éƒ½åœ¨ prompt ä¸­
- Prompt ä¸­çš„ tokens ä¹‹é—´**ä¸å— causal mask é™åˆ¶**
- å®ƒä»¬å¯ä»¥**å…¨è¿æ¥ attention**

#### è§„åˆ™ 2: Vision Tokens ä¸ç”Ÿæˆ Tokens

```
Vision Token i  â†’  Generation Token j:  âœ… å¯ä»¥ï¼ˆå¦‚æœ j > i çš„ä½ç½®ï¼‰
Generation Token j  â†’  Vision Token i:  âœ… å¯ä»¥ï¼ˆvision token åœ¨ prompt ä¸­ï¼‰
```

**åŸå› **ï¼š
- Vision tokens åœ¨ prompt ä¸­ï¼Œå§‹ç»ˆå¯è§
- ç”Ÿæˆçš„ tokens å¯ä»¥ attention åˆ°æ‰€æœ‰ prompt tokensï¼ˆåŒ…æ‹¬ vision tokensï¼‰

### ä¸æ˜¯ View-Wise Causal

**é‡è¦æ¾„æ¸…**ï¼šVision tokens **ä¸æ˜¯ view-wise causal**ï¼Œè€Œæ˜¯ï¼š

1. **Vision tokens ä¹‹é—´å…¨è¿æ¥**ï¼ˆéƒ½åœ¨ prompt ä¸­ï¼‰
2. **ç”Ÿæˆ tokens å¯¹ vision tokens å…¨è¿æ¥**ï¼ˆvision tokens åœ¨ prompt ä¸­ï¼‰
3. **åªæœ‰ç”Ÿæˆ tokens ä¹‹é—´æ˜¯ causal**ï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰

### å®Œæ•´çš„ Attention Mask ç»“æ„

å‡è®¾ï¼š
- Vision tokens: 16 å¼ å›¾åƒ Ã— 256 tokens/å›¾åƒ = 4096 tokens
- å†å²è½¨è¿¹ tokens: 48 tokens
- ç”¨æˆ·æŒ‡ä»¤ tokens: 20 tokens
- ç”Ÿæˆ tokens: 100 tokensï¼ˆCoC æ–‡æœ¬ï¼‰

**æ€»åºåˆ—é•¿åº¦**ï¼š4096 + 48 + 20 + 100 = 4264 tokens

**Attention Mask çŸ©é˜µ**ï¼š

**å½¢çŠ¶**ï¼š`(1, 1, 4264, 4264)` - (batch, heads, seq_len, seq_len)

**ç»“æ„ç¤ºæ„**ï¼ˆç®€åŒ–ï¼Œåªæ˜¾ç¤ºå…³é”®éƒ¨åˆ†ï¼‰ï¼š

```
                Vision    Traj    Text    Gen0  Gen1  Gen2  ...
                [V0...V15] [T0...] [U0...] [G0] [G1] [G2]  ...
Vision V0       [1...1]    [1...1] [1...1] [1]  [1]  [1]   ...  â† å…¨è¿æ¥
Vision V1       [1...1]    [1...1] [1...1] [1]  [1]  [1]   ...  â† å…¨è¿æ¥
...
Vision V15      [1...1]    [1...1] [1...1] [1]  [1]  [1]   ...  â† å…¨è¿æ¥
Traj T0         [1...1]    [1...1] [1...1] [1]  [1]  [1]   ...  â† å…¨è¿æ¥
...
Text U0         [1...1]    [1...1] [1...1] [1]  [1]  [1]   ...  â† å…¨è¿æ¥
...
Gen G0          [1...1]    [1...1] [1...1] [1]  [0]  [0]   ...  â† Causal
Gen G1          [1...1]    [1...1] [1...1] [1]  [1]  [0]   ...  â† Causal
Gen G2          [1...1]    [1...1] [1...1] [1]  [1]  [1]   ...  â† Causal
...
```

**è¯´æ˜**ï¼š
- `1` è¡¨ç¤ºå¯ä»¥ attentionï¼ˆmask = 0ï¼Œä¸å±è”½ï¼‰
- `0` è¡¨ç¤ºä¸èƒ½ attentionï¼ˆmask = -infï¼Œå±è”½ï¼‰

**å…³é”®è§‚å¯Ÿ**ï¼š
1. **Vision tokens è¡Œ**ï¼šæ‰€æœ‰ä½ç½®éƒ½æ˜¯ `1`ï¼ˆå…¨è¿æ¥ï¼‰
2. **Prompt tokens è¡Œ**ï¼šæ‰€æœ‰ä½ç½®éƒ½æ˜¯ `1`ï¼ˆå…¨è¿æ¥ï¼‰
3. **Generation tokens è¡Œ**ï¼š
   - å¯¹ prompt tokensï¼šéƒ½æ˜¯ `1`ï¼ˆå¯ä»¥çœ‹åˆ°æ‰€æœ‰ promptï¼‰
   - å¯¹ generation tokensï¼šcausal maskï¼ˆåªèƒ½çœ‹åˆ°ä¹‹å‰çš„ç”Ÿæˆ tokensï¼‰

---

## 15. View å’Œ Frame çš„é¡ºåºå…³ç³»

### å½“å‰å®ç°ï¼šView-First Ordering

**é¡ºåº**ï¼šå…ˆæŒ‰ Viewï¼ˆç›¸æœºï¼‰ï¼Œå†æŒ‰ Frameï¼ˆæ—¶é—´ï¼‰

```
[View0_Frame0, View0_Frame1, View0_Frame2, View0_Frame3,
 View1_Frame0, View1_Frame1, View1_Frame2, View1_Frame3,
 View2_Frame0, View2_Frame1, View2_Frame2, View2_Frame3,
 View3_Frame0, View3_Frame1, View3_Frame2, View3_Frame3]
```

**ä¼˜ç‚¹**ï¼š
- æ¯ä¸ªç›¸æœºçš„å¸§è¿ç»­ï¼Œä¾¿äºå»ºç«‹æ—¶åºå…³ç³»
- å®ç°ç®€å•ï¼ˆç›´æ¥ flattenï¼‰

**ç¼ºç‚¹**ï¼š
- åŒä¸€æ—¶åˆ»çš„ä¸åŒè§†è§’**ä¸è¿ç»­**
- æ¨¡å‹éœ€è¦è·¨è¶Šè¾ƒè¿œè·ç¦»æ‰èƒ½å…³è”åŒä¸€æ—¶åˆ»çš„å¤šè§†è§’ä¿¡æ¯

### æ›¿ä»£æ–¹æ¡ˆï¼šFrame-First Ordering

å¦‚æœä½¿ç”¨ Frame-First é¡ºåºï¼š

```
[View0_Frame0, View1_Frame0, View2_Frame0, View3_Frame0,  # æ—¶åˆ» t0-0.3s çš„æ‰€æœ‰è§†è§’
 View0_Frame1, View1_Frame1, View2_Frame1, View3_Frame1,  # æ—¶åˆ» t0-0.2s çš„æ‰€æœ‰è§†è§’
 View0_Frame2, View1_Frame2, View2_Frame2, View3_Frame2,  # æ—¶åˆ» t0-0.1s çš„æ‰€æœ‰è§†è§’
 View0_Frame3, View1_Frame3, View2_Frame3, View3_Frame3]  # æ—¶åˆ» t0 çš„æ‰€æœ‰è§†è§’
```

**ä¼˜ç‚¹**ï¼š
- åŒä¸€æ—¶åˆ»çš„å¤šè§†è§’ä¿¡æ¯è¿ç»­ï¼Œä¾¿äºèåˆ
- æ›´ç¬¦åˆ"å¤šè§†è§’åŒæ—¶è§‚å¯Ÿ"çš„ç‰©ç†ç›´è§‰

**ç¼ºç‚¹**ï¼š
- åŒä¸€ç›¸æœºçš„æ—¶åºä¿¡æ¯ä¸è¿ç»­
- éœ€è¦é‡æ–°ç»„ç»‡æ•°æ®

### ä¸ºä»€ä¹ˆé€‰æ‹© View-Firstï¼Ÿ

1. **å®ç°ç®€å•**ï¼šç›´æ¥ä½¿ç”¨ `flatten(0, 1)` å³å¯
2. **Attention æœºåˆ¶è¡¥å¿**ï¼šè™½ç„¶åŒä¸€æ—¶åˆ»çš„è§†è§’ä¸è¿ç»­ï¼Œä½† attention æœºåˆ¶å¯ä»¥è·¨è¶Šè·ç¦»å»ºç«‹å…³è”
3. **è®­ç»ƒæ•°æ®ä¸€è‡´æ€§**ï¼šè®­ç»ƒæ—¶å¯èƒ½å°±ä½¿ç”¨è¿™ç§é¡ºåºï¼Œä¿æŒä¸€è‡´æ€§

---

## 16. Vision Token Attention è¯æ®åˆ†æ

### ä»£ç è¯æ®

#### 1. Processor è¿”å›çš„ attention_mask

**ä»£ç ä½ç½®**ï¼š`test_inference.py:38-45`

```python
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
)
# inputs åŒ…å«: ['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw']
```

**å®é™…å†…å®¹**ï¼š
```python
# å®é™…æµ‹è¯•ç»“æœ
attention_mask shape: torch.Size([1, 75])
attention_mask sample: tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])
```

**è¯´æ˜**ï¼š
- è¿™ä¸ª `attention_mask` æ˜¯ **1D mask**ï¼Œç”¨äºæ ‡è®°æœ‰æ•ˆ token vs padding
- **ä¸æ˜¯ç”¨æ¥æ§åˆ¶ causal attention çš„**
- å®ƒåªæ˜¯å‘Šè¯‰æ¨¡å‹å“ªäº›ä½ç½®æ˜¯æœ‰æ•ˆçš„ï¼ˆ1ï¼‰å’Œå“ªäº›æ˜¯ paddingï¼ˆ0ï¼‰

#### 2. VLM Generate è°ƒç”¨

**ä»£ç ä½ç½®**ï¼š`alpamayo_r1.py:192-198`

```python
vlm_outputs = self.vlm.generate(
    input_ids=input_ids,
    generation_config=generation_config,
    stopping_criteria=stopping_criteria,
    logits_processor=logits_processor,
    **tokenized_data,  # åŒ…å« attention_mask, pixel_values, image_grid_thw
)
```

**å…³é”®è§‚å¯Ÿ**ï¼š
- **æ²¡æœ‰æ˜¾å¼è®¾ç½® `is_causal=False`**
- **æ²¡æœ‰æ˜¾å¼è®¾ç½®ç‰¹æ®Šçš„ attention mask**
- ä½¿ç”¨çš„æ˜¯ `vlm.generate()` çš„**é»˜è®¤è¡Œä¸º**

#### 3. é»˜è®¤è¡Œä¸ºæ¨æ–­

**æ ‡å‡† Transformer ç”Ÿæˆè¡Œä¸º**ï¼š
- `generate()` æ–¹æ³•å†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç† causal mask
- **Prompt éƒ¨åˆ†**ï¼ˆåŒ…æ‹¬ vision tokensï¼‰ï¼šä½¿ç”¨**å…¨è¿æ¥ attention**
- **ç”Ÿæˆéƒ¨åˆ†**ï¼šä½¿ç”¨ **causal mask**

**è¯æ®æ¥æº**ï¼š
1. Transformers åº“çš„æ ‡å‡†å®ç°
2. Qwen3-VL çš„æ¨¡å‹æ¶æ„ï¼ˆåŸºäº Transformer Decoderï¼‰
3. ä»£ç ä¸­æ²¡æœ‰æ˜¾å¼è¦†ç›–è¿™ä¸ªè¡Œä¸º

#### 4. é—´æ¥è¯æ®ï¼šExpert Model çš„å¯¹æ¯”

**ä»£ç ä½ç½®**ï¼š`alpamayo_r1.py:251-252`

```python
if self.config.expert_non_causal_attention:
    forward_kwargs["is_causal"] = False  # â­ æ˜¾å¼è®¾ç½®éå› æœ
```

**å¯¹æ¯”**ï¼š
- **Expert Model**ï¼š**æ˜¾å¼è®¾ç½®** `is_causal=False`
- **VLM Generate**ï¼š**æ²¡æœ‰æ˜¾å¼è®¾ç½®**ï¼Œä½¿ç”¨é»˜è®¤è¡Œä¸º

**æ¨æ–­**ï¼š
- å¦‚æœ VLM éœ€è¦éå› æœ attentionï¼Œåº”è¯¥åƒ Expert Model ä¸€æ ·æ˜¾å¼è®¾ç½®
- ä½† VLM çš„ prompt éƒ¨åˆ†ï¼ˆåŒ…æ‹¬ vision tokensï¼‰é»˜è®¤å°±æ˜¯å…¨è¿æ¥çš„
- åªæœ‰ç”Ÿæˆéƒ¨åˆ†ä½¿ç”¨ causal mask

### é‡è¦æ¾„æ¸…

**ä»£ç ä¸­æ²¡æœ‰æ˜¾å¼è¯æ®**è¯æ˜ vision tokens æ˜¯å…¨è¿æ¥çš„ã€‚è¿™æ˜¯åŸºäº **`vlm.generate()` çš„æ ‡å‡†è¡Œä¸ºæ¨æ–­**ã€‚

**æ­£ç¡®çš„è¡¨è¿°åº”è¯¥æ˜¯**ï¼š

1. **Prompt éƒ¨åˆ†ï¼ˆåŒ…æ‹¬ vision tokensï¼‰**ï¼š
   - åœ¨ `vlm.generate()` çš„ prompt é˜¶æ®µï¼Œä½¿ç”¨**å…¨è¿æ¥ attention**
   - è¿™æ˜¯ Transformers åº“çš„é»˜è®¤è¡Œä¸º
   - **ä»£ç ä¸­æ²¡æœ‰æ˜¾å¼è®¾ç½®ï¼Œä½†ä¹Ÿæ²¡æœ‰è¦†ç›–è¿™ä¸ªè¡Œä¸º**

2. **ç”Ÿæˆéƒ¨åˆ†**ï¼š
   - ä½¿ç”¨ **causal mask**ï¼ˆè¿™æ˜¯ `generate()` çš„é»˜è®¤è¡Œä¸ºï¼‰

### å¦‚ä½•éªŒè¯ï¼Ÿ

è¦çœŸæ­£éªŒè¯ vision tokens æ˜¯å¦å…¨è¿æ¥ï¼Œéœ€è¦ï¼š

1. **æŸ¥çœ‹ Qwen3-VL çš„æºç **ï¼š
   ```python
   # åœ¨ transformers åº“ä¸­æŸ¥çœ‹
   # transformers/models/qwen3_vl/modeling_qwen3_vl.py
   ```

2. **è¿è¡Œæ—¶æ£€æŸ¥**ï¼š
   ```python
   # åœ¨ generate è¿‡ç¨‹ä¸­æ‰“å° attention mask
   # æˆ–è€…ä½¿ç”¨ hook æŸ¥çœ‹å®é™…çš„ attention æƒé‡
   ```

3. **æŸ¥çœ‹æ¨¡å‹é…ç½®**ï¼š
   ```python
   # æ£€æŸ¥ vlm.config ä¸­æ˜¯å¦æœ‰ç›¸å…³è®¾ç½®
   ```

### ä»£ç è¯æ®æ€»ç»“

| è¯æ®ç±»å‹ | è¯æ®å†…å®¹ | å¼ºåº¦ |
|---------|---------|------|
| **ç›´æ¥è¯æ®** | ä»£ç ä¸­æ˜¾å¼è®¾ç½® vision tokens å…¨è¿æ¥ | âŒ æ—  |
| **é—´æ¥è¯æ®** | `vlm.generate()` é»˜è®¤è¡Œä¸º | âœ… å¼ºï¼ˆåŸºäºæ ‡å‡†å®ç°ï¼‰ |
| **å¯¹æ¯”è¯æ®** | Expert Model æ˜¾å¼è®¾ç½® `is_causal=False` | âœ… ä¸­ç­‰ï¼ˆè¯´æ˜å¦‚æœéœ€è¦éå› æœä¼šæ˜¾å¼è®¾ç½®ï¼‰ |
| **Processor è¾“å‡º** | `attention_mask` åªæ˜¯æ ‡è®°æœ‰æ•ˆ token | âœ… å¼±ï¼ˆä¸æ§åˆ¶ causalï¼‰ |

### æœ€ç»ˆç»“è®º

1. **ä»£ç ä¸­æ²¡æœ‰æ˜¾å¼è¯æ®**è¯æ˜ vision tokens æ˜¯å…¨è¿æ¥çš„
2. **åŸºäºæ ‡å‡†è¡Œä¸ºæ¨æ–­**ï¼švision tokens åœ¨ prompt é˜¶æ®µåº”è¯¥æ˜¯å…¨è¿æ¥çš„
3. **éœ€è¦è¿›ä¸€æ­¥éªŒè¯**ï¼šæŸ¥çœ‹ Qwen3-VL æºç æˆ–è¿è¡Œæ—¶æ£€æŸ¥

---

# æ€»ç»“

## å…³é”®è®¾è®¡ç‰¹ç‚¹

### 1. æ··åˆæ¶æ„
- **VLM**: è´Ÿè´£è§†è§‰ç†è§£å’Œ Chain-of-Causation æ¨ç†ç”Ÿæˆ
- **Expert Model**: ä¸“é—¨å¤„ç†åŠ¨ä½œåºåˆ—ï¼Œå¤ç”¨ VLM çš„ä¸Šä¸‹æ–‡

### 2. KV Cache å¤ç”¨
- VLM ç”Ÿæˆçš„ KV cache è¢«ä¸“å®¶æ¨¡å‹å¤ç”¨ï¼Œé¿å…é‡å¤è®¡ç®—
- å®ç°é«˜æ•ˆçš„ä¸Šä¸‹æ–‡ä¼ é€’

### 3. è½¨è¿¹Tokenç³»ç»Ÿ
- å†å²è½¨è¿¹ç¼–ç ä¸ºç¦»æ•£tokenï¼Œæ— ç¼èå…¥è¯­è¨€æ¨¡å‹
- æœªæ¥è½¨è¿¹é€šè¿‡æ‰©æ•£æ¨¡å‹åœ¨è¿ç»­åŠ¨ä½œç©ºé—´ä¸­ç”Ÿæˆ

### 4. ä¸¤é˜¶æ®µç”Ÿæˆ
- **é˜¶æ®µ1**: VLM ç”Ÿæˆæ–‡æœ¬æ¨ç†ï¼ˆCoCï¼‰
- **é˜¶æ®µ2**: Expert + Diffusion ç”Ÿæˆè½¨è¿¹åŠ¨ä½œ

### 5. å¯è§£é‡Šæ€§
- æ¯ä¸ªé¢„æµ‹è½¨è¿¹éƒ½ä¼´éš Chain-of-Causation æ¨ç†æ–‡æœ¬
- æä¾›å†³ç­–è¿‡ç¨‹çš„è‡ªç„¶è¯­è¨€è§£é‡Š

### 6. Attention æœºåˆ¶
- **VLM**: Causal attentionï¼ˆè‡ªå›å½’ç”Ÿæˆï¼‰
- **Expert Model**: Non-causal attentionï¼ˆå¹¶è¡Œå¤„ç†ï¼‰
- **Vision Tokens**: åœ¨ prompt ä¸­ï¼Œå…¨è¿æ¥ attention

---

# ç¬¬äº”éƒ¨åˆ†ï¼šå›¾åƒå¤„ç†ä¸ Vision Encoder

## 17. å›¾åƒå¤„ç†æµç¨‹è¯¦è§£

æœ¬æ–‡æ¡£è¯¦ç»†è¯´æ˜ processor å¦‚ä½•å°†è¾“å…¥å›¾åƒä» `[4, 4, 3, 1080, 1920]` å¤„ç†æˆ `pixel_values [11520, 1536]`ã€‚

### 17.1 è¾“å…¥æ•°æ®å½¢çŠ¶

#### åŸå§‹è¾“å…¥

```python
# load_physical_aiavdataset.py è¿”å›
image_frames: (N_cameras=4, num_frames=4, C=3, H=1080, W=1920)
```

**å«ä¹‰**ï¼š
- `4` ä¸ªç›¸æœºï¼ˆviewsï¼‰
- æ¯ä¸ªç›¸æœº `4` å¸§ï¼ˆframesï¼‰
- æ¯å¸§å›¾åƒï¼š`(3, 1080, 1920)` - RGB é€šé“ï¼Œé«˜åº¦ 1080ï¼Œå®½åº¦ 1920

#### å›¾åƒå±•å¹³

```python
# test_inference.py ç¬¬ 33 è¡Œ
messages = helper.create_message(data["image_frames"].flatten(0, 1))
```

**`flatten(0, 1)` çš„æ•ˆæœ**ï¼š
- è¾“å…¥ï¼š`(4, 4, 3, 1080, 1920)`
- è¾“å‡ºï¼š`(16, 3, 1080, 1920)`

**å±•å¹³é¡ºåº**ï¼ˆView-Firstï¼‰ï¼š
```
[Camera0_Frame0, Camera0_Frame1, Camera0_Frame2, Camera0_Frame3,
 Camera1_Frame0, Camera1_Frame1, Camera1_Frame2, Camera1_Frame3,
 Camera2_Frame0, Camera2_Frame1, Camera2_Frame2, Camera2_Frame3,
 Camera3_Frame0, Camera3_Frame1, Camera3_Frame2, Camera3_Frame3]
```

### 17.2 Processor å¤„ç†æµç¨‹

#### Message æ„å»º

```python
# helper.py ç¬¬ 28-67 è¡Œ
def create_message(frames: torch.Tensor):
    """frames: (16, 3, 1080, 1920)"""
    return [
        {
            "role": "user",
            "content": [
                {"type": "image", "image": frame} for frame in frames  # 16 å¼ å›¾åƒ
            ] + [{"type": "text", "text": "..."}]
        }
    ]
```

**å…³é”®ç‚¹**ï¼š
- æ¯å¼ å›¾åƒä½œä¸ºç‹¬ç«‹çš„ `{"type": "image", "image": frame}` é¡¹
- å›¾åƒé¡ºåºï¼šView-Firstï¼ˆå…ˆæŒ‰ç›¸æœºï¼Œå†æŒ‰å¸§ï¼‰

#### Processor è°ƒç”¨

```python
# test_inference.py ç¬¬ 38-45 è¡Œ
processor = helper.get_processor(model.tokenizer)  # Qwen3-VL processor

inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
)
```

**Processor é…ç½®**ï¼š
```python
# helper.py ç¬¬ 70-79 è¡Œ
processor = AutoProcessor.from_pretrained(
    "Qwen/Qwen3-VL-2B-Instruct",
    min_pixels=163840,   # æœ€å°åƒç´ æ•°
    max_pixels=196608,   # æœ€å¤§åƒç´ æ•°
)
```

### 17.3 Processor å†…éƒ¨å¤„ç†æ­¥éª¤

#### æ­¥éª¤ 1: å›¾åƒé¢„å¤„ç†

Qwen3-VL Processor å¯¹æ¯å¼ å›¾åƒæ‰§è¡Œä»¥ä¸‹æ“ä½œï¼š

##### Resizeï¼ˆè°ƒæ•´å¤§å°ï¼‰

**ç›®æ ‡**ï¼šå°†å›¾åƒè°ƒæ•´åˆ°ç›®æ ‡åƒç´ èŒƒå›´å†…ï¼ˆ163840 - 196608 åƒç´ ï¼‰

**åŸå§‹å›¾åƒ**ï¼š
- å°ºå¯¸ï¼š`(1080, 1920)`
- åƒç´ æ•°ï¼š`1080 Ã— 1920 = 2,073,600`

**ç¼©æ”¾è®¡ç®—**ï¼š
```python
target_pixels = (min_pixels + max_pixels) / 2  # â‰ˆ 180,224
scale_factor = sqrt(target_pixels / original_pixels)  # â‰ˆ 0.295
```

**ç¼©æ”¾åå°ºå¯¸**ï¼š
- ä¿æŒå®½é«˜æ¯”ç¼©æ”¾
- ç›®æ ‡åƒç´ æ•°ï¼šçº¦ 180,224 åƒç´ 
- **å®é™…å°ºå¯¸**ï¼š`(320, 576)` åƒç´ ï¼ˆæœ€æ¥è¿‘åŸå§‹å®½é«˜æ¯” 1.78ï¼‰
- éªŒè¯ï¼š`320 Ã— 576 = 184,320` åƒç´  âœ“ï¼ˆåœ¨ç›®æ ‡èŒƒå›´å†…ï¼‰

##### Patch åˆ†å‰²

**Patch å¤§å°**ï¼š`16 Ã— 16` åƒç´ 

**æ¯å¼ å›¾åƒçš„ Patches**ï¼š
- 720 patches per image
- å›¾åƒå°ºå¯¸ï¼š`(320, 576)` åƒç´ 
- H patchesï¼š`320 / 16 = 20`
- W patchesï¼š`576 / 16 = 36`
- Total patchesï¼š`20 Ã— 36 = 720` âœ“

**éªŒè¯**ï¼š
```
16 å¼ å›¾åƒ Ã— 720 patches/å›¾åƒ = 11,520 patches
è¾“å‡º pixel_values: [11520, 1536] âœ“
```

##### Vision Encoder å¤„ç†

**é‡è¦**ï¼šVision Encoder åœ¨ **processor å†…éƒ¨**è¢«è°ƒç”¨ï¼

æ¯å¼ å›¾åƒçš„ patches é€šè¿‡ Vision Encoderï¼ˆQwen3VLVisionModelï¼‰ï¼š

```
è¾“å…¥: (720, 3, 16, 16)  # 720 ä¸ª patchesï¼Œæ¯ä¸ª 16Ã—16Ã—3
    â†“
Vision Encoder (Qwen3VLVisionModel)
    â”œâ”€â”€ Patch Embedding
    â”œâ”€â”€ Position Embedding  
    â”œâ”€â”€ Transformer Layers
    â””â”€â”€ è¾“å‡º: (720, 1152)  # Vision Encoder çš„è¾“å‡ºç»´åº¦æ˜¯ 1152
    â†“
æŠ•å½±åˆ° 1536 ç»´ï¼ˆå¦‚æœéœ€è¦ï¼‰
    â†“
æœ€ç»ˆ: (720, 1536)  # 720 ä¸ª patch embeddingsï¼Œæ¯ä¸ª 1536 ç»´
```

**å…³é”®å‚æ•°**ï¼š
- `patch_size = 16`ï¼šæ¯ä¸ª patch æ˜¯ 16Ã—16 åƒç´ 
- Vision Encoder `hidden_size = 1152`ï¼šVision Encoder çš„è¾“å‡ºç»´åº¦
- æœ€ç»ˆ `hidden_size = 1536`ï¼šæŠ•å½±åçš„ç»´åº¦ï¼ˆä¸æ–‡æœ¬æ¨¡å‹åŒ¹é…ï¼‰

**è°ƒç”¨ä½ç½®**ï¼š
- Vision Encoder åœ¨ `processor.apply_chat_template()` **å†…éƒ¨**è¢«è°ƒç”¨
- ä¸æ˜¯åœ¨ `vlm.generate()` å†…éƒ¨è°ƒç”¨
- `pixel_values` è¾“å‡ºæ—¶å·²ç»æ˜¯ embeddings

#### æ­¥éª¤ 2: æ‰¹é‡å¤„ç†

**æ‰€æœ‰å›¾åƒçš„å¤„ç†**ï¼š
```
16 å¼ å›¾åƒ Ã— 720 patches/å›¾åƒ = 11,520 patches
æ¯ä¸ª patch: 1536 ç»´ç‰¹å¾
æœ€ç»ˆè¾“å‡º: (11520, 1536)
```

### 17.4 å½¢çŠ¶å˜æ¢è¯¦è§£

#### å®Œæ•´å˜æ¢æµç¨‹

```
è¾“å…¥: [4, 4, 3, 1080, 1920]
    â†“ flatten(0, 1)
[16, 3, 1080, 1920]  # 16 å¼ å›¾åƒ
    â†“ create_message
messages with 16 images
    â†“ processor.apply_chat_template
    â”œâ”€â”€ å¯¹æ¯å¼ å›¾åƒ:
    â”‚   â”œâ”€â”€ Resize to target pixels (163840-196608)
    â”‚   â”œâ”€â”€ Split into patches (16Ã—16 each)
    â”‚   â””â”€â”€ Encode with Vision Encoder
    â”‚       â†’ (720, 1536) per image
    â””â”€â”€ Concatenate all images
        â†’ (11520, 1536)
```

#### è¯¦ç»†è®¡ç®—

**æ¯å¼ å›¾åƒçš„å¤„ç†**ï¼š

**è¾“å…¥**ï¼š`(3, 1080, 1920)`

**æ­¥éª¤ 1: Resize**
- ç›®æ ‡åƒç´ ï¼šçº¦ 180,224ï¼ˆåœ¨ 163840-196608 èŒƒå›´å†…ï¼‰
- ä¿æŒå®½é«˜æ¯”ï¼š`1080:1920 â‰ˆ 1:1.78`
- **å®é™…å°ºå¯¸**ï¼š`(320, 576)` åƒç´ 
- éªŒè¯ï¼š`320 Ã— 576 = 184,320` âœ“ï¼ˆåœ¨ç›®æ ‡èŒƒå›´å†…ï¼‰
- å®½é«˜æ¯”ï¼š`576 / 320 = 1.80` âœ“ï¼ˆæ¥è¿‘åŸå§‹ 1.78ï¼‰

**æ­¥éª¤ 2: Patch åˆ†å‰²**
- Patch å¤§å°ï¼š`16 Ã— 16`
- H patchesï¼š`320 / 16 = 20`
- W patchesï¼š`576 / 16 = 36`
- Total patchesï¼š`20 Ã— 36 = 720` âœ“

**æ­¥éª¤ 3: Vision Encoder**
- è¾“å…¥ï¼š`(720, 3, 16, 16)` - 720 ä¸ª patches
- è¾“å‡ºï¼š`(720, 1536)` - 720 ä¸ª patch embeddings

**æ‰¹é‡å¤„ç†**ï¼š

**16 å¼ å›¾åƒ**ï¼š
```
16 Ã— 720 = 11,520 patches
æ¯ä¸ª patch: 1536 ç»´
æœ€ç»ˆ: (11520, 1536) âœ“
```

### 17.5 å…³é”®å‚æ•°

#### Processor é…ç½®

```python
# helper.py
MIN_PIXELS = 163840
MAX_PIXELS = 196608
BASE_PROCESSOR_NAME = "Qwen/Qwen3-VL-2B-Instruct"

processor = AutoProcessor.from_pretrained(
    BASE_PROCESSOR_NAME,
    min_pixels=MIN_PIXELS,
    max_pixels=MAX_PIXELS,
)
```

#### Image Processor å‚æ•°

```python
# Qwen3-VL Image Processor
patch_size = 16              # æ¯ä¸ª patch 16Ã—16 åƒç´ 
hidden_size = 1536           # Vision Encoder è¾“å‡ºç»´åº¦
min_pixels = 163840          # æœ€å°åƒç´ æ•°
max_pixels = 196608          # æœ€å¤§åƒç´ æ•°
```

#### å®é™…å¤„ç†ç»“æœ

- **æ¯å¼ å›¾åƒ patches**ï¼š720
- **æ¯å¼ å›¾åƒåƒç´ **ï¼š184,320ï¼ˆåœ¨ç›®æ ‡èŒƒå›´å†…ï¼‰
- **å›¾åƒå°ºå¯¸**ï¼š`(320, 576)` åƒç´ 
- **Patch å¸ƒå±€**ï¼š`20 Ã— 36` patches per imageï¼ˆ20 è¡Œ Ã— 36 åˆ—ï¼‰

### 17.6 å½¢çŠ¶å˜æ¢æ€»ç»“

| é˜¶æ®µ | å½¢çŠ¶ | è¯´æ˜ |
|------|------|------|
| **åŸå§‹è¾“å…¥** | `(4, 4, 3, 1080, 1920)` | 4ä¸ªç›¸æœºÃ—4å¸§ |
| **å±•å¹³å** | `(16, 3, 1080, 1920)` | 16å¼ å›¾åƒ |
| **Resize** | `(16, 3, 320, 576)` | è°ƒæ•´åˆ°ç›®æ ‡åƒç´ ï¼ˆ184,320åƒç´ ï¼‰ |
| **Patch åˆ†å‰²** | `(16, 720, 3, 16, 16)` | æ¯å¼ å›¾åƒ720ä¸ªpatchesï¼ˆ20Ã—36ï¼‰ |
| **Vision Encoder** | `(16, 720, 1536)` | æ¯å¼ å›¾åƒ720ä¸ªembeddings |
| **æœ€ç»ˆè¾“å‡º** | `(11520, 1536)` | æ‰€æœ‰å›¾åƒçš„patchesæ‹¼æ¥ |

### 17.7 å…³é”®æ•°å­—

- **è¾“å…¥å›¾åƒæ•°**ï¼š16 å¼ ï¼ˆ4 ç›¸æœº Ã— 4 å¸§ï¼‰
- **æ¯å¼ å›¾åƒ patches**ï¼š720
- **æ€» patches**ï¼š11,520
- **æ¯ä¸ª patch ç»´åº¦**ï¼š1,536
- **Patch å¤§å°**ï¼š16 Ã— 16 åƒç´ 
- **ç›®æ ‡åƒç´ èŒƒå›´**ï¼š163,840 - 196,608

---

## 18. Vision Encoder ä½ç½®ä¸è°ƒç”¨

æœ¬æ–‡æ¡£è¯´æ˜ Vision Encoder åœ¨ Alpamayo ä»£ç ä¸­çš„ä½ç½®å’Œè®¿é—®æ–¹å¼ã€‚

### 18.1 Vision Encoder çš„ä½ç½®

#### åœ¨ Qwen3-VL æ¨¡å‹ä¸­çš„ä½ç½®

Vision Encoder æ˜¯ **Qwen3-VL æ¨¡å‹çš„ä¸€ä¸ªç»„ä»¶**ï¼Œä½äºæ¨¡å‹å†…éƒ¨ã€‚

**æ¨¡å‹ç»“æ„**ï¼š
```
Qwen3VLForConditionalGeneration
  â”œâ”€â”€ visual (Qwen3VLVisionModel)  â­ Vision Encoder åœ¨è¿™é‡Œ
  â””â”€â”€ model (Qwen3VLModel)
      â””â”€â”€ language_model (Text Model)
```

#### åœ¨ Alpamayo ä»£ç ä¸­çš„è®¿é—®è·¯å¾„

**ä»£ç ä½ç½®**ï¼š`base_model.py` ç¬¬ 381 è¡Œ

```python
# base_model.py ç¬¬ 367-381 è¡Œ
def _initialize_qwenvl3_vlm(self, config: ReasoningVLAConfig) -> None:
    """Initialize Qwen3-VL VLM backbone."""
    vlm_config = Qwen3VLConfig.from_pretrained(
        config.vlm_name_or_path,
        dtype=config.model_dtype,
        attn_implementation=config.attn_implementation,
    )
    self.vlm = Qwen3VLForConditionalGeneration(vlm_config)
```

**è®¿é—® Vision Encoder**ï¼š
```python
# åœ¨ Alpamayo ä»£ç ä¸­
vision_encoder = model.vlm.visual  # â­ æ­£ç¡®çš„è·¯å¾„
print(type(vision_encoder))  # <class 'transformers.models.qwen3_vl.modeling_qwen3_vl.Qwen3VLVisionModel'>
```

### 18.2 Vision Encoder çš„é…ç½®

**Vision Config**ï¼š
```python
# Qwen3-VL Vision Config
vision_config = {
    "hidden_size": 1152,      # Vision Encoder çš„éšè—ç»´åº¦
    "patch_size": 16,         # Patch å¤§å°
    # ... å…¶ä»–é…ç½®
}
```

**æ³¨æ„**ï¼š
- Vision Encoder çš„è¾“å‡ºç»´åº¦æ˜¯ **1152**ï¼ˆä¸æ˜¯ 1536ï¼‰
- 1536 æ˜¯ç»è¿‡æŠ•å½±åçš„ç»´åº¦ï¼Œæˆ–è€…æ˜¯æ–‡æœ¬æ¨¡å‹çš„éšè—ç»´åº¦

### 18.3 Vision Encoder çš„å·¥ä½œæµç¨‹

**å®é™…æµç¨‹**ï¼ˆåœ¨ processor å†…éƒ¨ï¼‰ï¼š

```
è¾“å…¥: åŸå§‹å›¾åƒ [16, 3, 1080, 1920]
    â†“ processor.image_processor
    â”œâ”€â”€ Resize: (1080, 1920) â†’ (320, 576)
    â”œâ”€â”€ Patch åˆ†å‰²: 20 Ã— 36 = 720 patches per image
    â””â”€â”€ è¾“å‡º: [16, 720, 3, 16, 16]  # 16å¼ å›¾åƒï¼Œæ¯å¼ 720ä¸ªpatches
    â†“ processor.apply_chat_template (å†…éƒ¨)
    â”œâ”€â”€ è°ƒç”¨ Vision Encoder (model.vlm.visual)
    â”‚   â”œâ”€â”€ Patch Embedding
    â”‚   â”œâ”€â”€ Position Embedding
    â”‚   â”œâ”€â”€ Transformer Layers
    â”‚   â””â”€â”€ è¾“å‡º: [11520, 1152]  # Vision tokens (1152 æ˜¯ vision encoder è¾“å‡º)
    â””â”€â”€ æŠ•å½±åˆ° 1536 ç»´ï¼ˆå¦‚æœéœ€è¦ï¼‰
    â†“
æœ€ç»ˆè¾“å‡º: pixel_values [11520, 1536]  # â­ å·²ç»æ˜¯ vision encoder çš„è¾“å‡º
    â†“
ä¼ é€’ç»™ vlm.generate()
    â”œâ”€â”€ ä¸ text tokens èåˆ
    â””â”€â”€ è¿›è¡Œè‡ªå›å½’ç”Ÿæˆ
```

**å…³é”®ç‚¹**ï¼š
- Vision Encoder åœ¨ `processor.apply_chat_template()` **å†…éƒ¨**è¢«è°ƒç”¨
- ä¸æ˜¯åœ¨ `vlm.generate()` å†…éƒ¨è°ƒç”¨
- `pixel_values` è¿›å…¥ `sample_trajectories_from_data_with_vlm_rollout` æ—¶å·²ç»æ˜¯ embeddings

### 18.4 ä»£ç ä¸­çš„ä½¿ç”¨

**é‡è¦å‘ç°**ï¼šVision Encoder åœ¨ **processor é˜¶æ®µ**å°±è¢«è°ƒç”¨äº†ï¼

**åœ¨ Processor é˜¶æ®µ**ï¼š

```python
# test_inference.py ç¬¬ 38-45 è¡Œ
inputs = processor.apply_chat_template(
    messages,
    tokenize=True,
    return_dict=True,
    return_tensors="pt",
)
# inputs["pixel_values"] å·²ç»æ˜¯ [11520, 1536] â­ Vision Encoder å·²æ‰§è¡Œ
```

**å…³é”®è¯æ®**ï¼š
- `pixel_values` çš„å½¢çŠ¶æ˜¯ `[11520, 1536]`
- `1536` æ˜¯ vision encoder çš„è¾“å‡ºç»´åº¦ï¼ˆä¸æ˜¯åŸå§‹å›¾åƒï¼‰
- è¿™è¯´æ˜ **vision encoder åœ¨ `processor.apply_chat_template()` å†…éƒ¨è¢«è°ƒç”¨**

**åœ¨ VLM Generate æ—¶**ï¼š

```python
# alpamayo_r1.py ç¬¬ 192-198 è¡Œ
vlm_outputs = self.vlm.generate(
    input_ids=input_ids,
    pixel_values=pixel_values,  # â­ å·²ç»æ˜¯ vision encoder çš„è¾“å‡º
    **tokenized_data,
)
```

**å®Œæ•´æµç¨‹**ï¼š
1. `processor.apply_chat_template()` 
   - é¢„å¤„ç†å›¾åƒï¼ˆresize, normalizeï¼‰
   - **è°ƒç”¨ Vision Encoder ç¼–ç å›¾åƒ** â­
   - è¾“å‡º `pixel_values: [11520, 1536]`ï¼ˆå·²ç»æ˜¯ embeddingsï¼‰
2. `vlm.generate()` 
   - æ¥æ”¶å·²ç»ç¼–ç çš„ `pixel_values`
   - å°† vision embeddings ä¸ text tokens èåˆ
   - è¿›è¡Œè‡ªå›å½’ç”Ÿæˆ

### 18.5 æŸ¥çœ‹ Vision Encoder çš„æ–¹æ³•

```python
# åœ¨ Python ä¸­æŸ¥çœ‹
model = AlpamayoR1.from_pretrained("nvidia/Alpamayo-R1-10B")

# è®¿é—® Vision Encoder
vision_encoder = model.vlm.visual  # â­ æ­£ç¡®çš„è·¯å¾„
print(type(vision_encoder))  # <class 'transformers.models.qwen3_vl.modeling_qwen3_vl.Qwen3VLVisionModel'>
print(vision_encoder)

# æŸ¥çœ‹é…ç½®
print(model.vlm.config.vision_config)
```

### 18.6 Vision Encoder ä½ç½®æ€»ç»“

| ä½ç½® | è·¯å¾„ | è¯´æ˜ |
|------|------|------|
| **åœ¨ Qwen3-VL ä¸­** | `model.visual` | Vision Encoder ç»„ä»¶ï¼ˆQwen3VLVisionModelï¼‰ |
| **åœ¨ Alpamayo ä¸­** | `self.vlm.visual` | é€šè¿‡ VLM è®¿é—® |
| **é…ç½®** | `self.vlm.config.vision_config` | Vision Encoder é…ç½® |

### 18.7 å…³é”®ä»£ç ä½ç½®

1. **VLM åˆå§‹åŒ–**ï¼š`base_model.py:367-381`
   ```python
   self.vlm = Qwen3VLForConditionalGeneration(vlm_config)
   ```

2. **Vision Encoder è°ƒç”¨**ï¼šåœ¨ `processor.apply_chat_template()` å†…éƒ¨ â­
   ```python
   inputs = processor.apply_chat_template(messages, ...)
   # inputs["pixel_values"] å·²ç»æ˜¯ [11520, 1536]ï¼ŒVision Encoder å·²æ‰§è¡Œ
   ```

3. **VLM Generate ä½¿ç”¨å·²ç¼–ç çš„ pixel_values**ï¼š`alpamayo_r1.py:192-198`
   ```python
   vlm_outputs = self.vlm.generate(
       pixel_values=pixel_values,  # å·²ç»æ˜¯ vision encoder çš„è¾“å‡º
       ...
   )
   ```

---

## å‚è€ƒèµ„æ–™

- Alpamayo ä»£ç åº“ï¼š`src/alpamayo_r1/` ç›®å½•
- Qwen3-VL æ–‡æ¡£ï¼šhttps://huggingface.co/Qwen/Qwen3-VL-8B-Instruct
- Transformers åº“æ–‡æ¡£
- Alpamayo è®ºæ–‡ï¼šhttps://arxiv.org/abs/2511.00088

---

**æ–‡æ¡£ç‰ˆæœ¬**: 1.1  
**æœ€åæ›´æ–°**: 2025-01-28  
**æ•´åˆå†…å®¹**: æ¶æ„æ–‡æ¡£ã€Expert Model è¯¦è§£ã€Attention æœºåˆ¶åˆ†æã€Vision Token åˆ†æã€å›¾åƒå¤„ç†æµç¨‹ã€Vision Encoder ä½ç½®è¯´æ˜
